---
title: "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs"
date: 2026-02-10
arxiv: "2602.08241v1"
category: "cs.AI"
model: "google/gemma-3-27b-it:free"
---

# Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li |
| **ë°œí‘œì¼** | 2026-02-09 |
| **arXiv** | [2602.08241v1](https://arxiv.org/pdf/2602.08241v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.AI |

---

## ğŸ“ ì´ˆë¡ (Abstract)

While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

---

## ğŸ” AI ë¶„ì„

## ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ Multimodal Large Language Models (MLLMs)ì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´ visual attentionì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ê¸°ì¡´ MLLMë“¤ì´ ì¶”ë¡  ê³¼ì •ì—ì„œ ì´ˆê¸° visual misalignmentë¥¼ ìˆ˜ì •í•˜ì§€ ëª»í•´ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤ëŠ” ì ì„ ì§€ì í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reinforcement learning (RL) ê¸°ë°˜ì˜ SAYO ëª¨ë¸ì„ ì œì•ˆí•˜ë©°, region-level visual attention rewardë¥¼ í†µí•´ visually grounded reasoning stepì— ëŒ€í•œ credit assignmentë¥¼ ê°•í™”í•©ë‹ˆë‹¤. SAYOëŠ” ë‹¤ì–‘í•œ multimodal benchmarkì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, ì•ˆì •ì ì¸ visual attention policy í•™ìŠµì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ê¸°ì¡´ MLLM ì—°êµ¬ë“¤ì´ ì£¼ë¡œ textual reasoning trajectoryì— ì§‘ì¤‘í•œ ë°˜ë©´, ë³¸ ë…¼ë¬¸ì€ **visual attentionì˜ í•™ìŠµê³¼ ê°œì„ ì— ì´ˆì ì„ ë§ì¶˜ ì **ì´ ê°€ì¥ í° noveltyì…ë‹ˆë‹¤. íŠ¹íˆ, **reinforcement learningì„ í™œìš©í•˜ì—¬ visual attentionì— ì§ì ‘ì ì¸ rewardë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹**ì€ MLLMì˜ visual grounding ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì´ visual feature extractionì— ì§‘ì¤‘í–ˆë‹¤ë©´, SAYOëŠ” ì¶”ì¶œëœ featureë¥¼ LLMì´ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ì— ëŒ€í•œ attention ë©”ì»¤ë‹ˆì¦˜ ìì²´ë¥¼ ê°•í™”í•œë‹¤ëŠ” ì ì—ì„œ ì°¨ë³„ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ë¬¸ì œ ì •ì˜ì˜ ëª…í™•ì„±:** MLLMì˜ ì¶”ë¡  ê³¼ì •ì—ì„œ visual misalignmentê°€ ì˜¤ë¥˜ì˜ ì£¼ìš” ì›ì¸ì„ì„ ëª…í™•í•˜ê²Œ ë¶„ì„í•˜ê³  ì œì‹œí–ˆìŠµë‹ˆë‹¤.
2. **íš¨ê³¼ì ì¸ í•´ê²°ì±… ì œì‹œ:** Reinforcement learning ê¸°ë°˜ì˜ SAYO ëª¨ë¸ì„ í†µí•´ visual attention í•™ìŠµì„ ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì œì‹œí•˜ê³ , ì‹¤í—˜ì ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
3. **ë‹¤ì–‘í•œ benchmarkì—ì„œì˜ ê²€ì¦:** ë‹¤ì–‘í•œ multimodal benchmarkì—ì„œ SAYOì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ì—¬ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **RL í•™ìŠµì˜ ë³µì¡ì„±:** Reinforcement learningì€ í•™ìŠµ ê³¼ì •ì´ ë¶ˆì•ˆì •í•˜ê³ , reward function ì„¤ê³„ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. SAYOì˜ reward functionì´ ëª¨ë“  ìƒí™©ì— ìµœì í™”ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.
2. **Region-level attentionì˜ í•œê³„:** Region-level attentionì€ ì´ë¯¸ì§€ ì „ì²´ì˜ ë§¥ë½ì„ íŒŒì•…í•˜ëŠ” ë° í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì„¸ë°€í•œ attention ë©”ì»¤ë‹ˆì¦˜ (ì˜ˆ: pixel-level attention)ê³¼ì˜ ë¹„êµ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
3. **Scientific ë„ë©”ì¸ ì ìš© ê°€ëŠ¥ì„±:** ë³¸ ë…¼ë¬¸ì€ ì¼ë°˜ì ì¸ multimodal benchmarkì—ì„œ ì„±ëŠ¥ì„ ê²€ì¦í–ˆì§€ë§Œ, Scientific ë„ë©”ì¸ì—ì„œì˜ ì„±ëŠ¥ì€ ëª…í™•í•˜ê²Œ ì œì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Scientific ë„ë©”ì¸ì˜ ë³µì¡í•œ ì‹œê° ì •ë³´ (ì˜ˆ: ê·¸ë˜í”„, ì°¨íŠ¸, í™”í•™ êµ¬ì¡°ì‹)ì— ëŒ€í•œ SAYOì˜ ì ìš© ê°€ëŠ¥ì„±ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì œê°€ í˜„ì¬ ì§„í–‰í•˜ê³  ìˆëŠ” Vision encoderì™€ Text LLM ê°„ì˜ ì •ë ¬(alignment) ì—°êµ¬ì— ë§¤ìš° ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, SAYOê°€ visual attentionì„ ê°•í™”í•˜ëŠ” ë°©ì‹ì€ vision encoderì—ì„œ ì¶”ì¶œëœ featureë¥¼ LLMì´ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ë„ë¡ ë•ëŠ” ì¤‘ìš”í•œ ì•„ì´ë””ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ ì—°êµ¬ì— ìˆì–´ì„œ, visual attentionì˜ ì¤‘ìš”ì„±ì„ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°í•˜ë©°, ì œê°€ ì¶”êµ¬í•˜ëŠ” ë°©í–¥ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **Scientific ë„ë©”ì¸ íŠ¹í™” SAYO ëª¨ë¸ ê°œë°œ:** Scientific ë„ë©”ì¸ì˜ íŠ¹ì§•ì ì¸ ì‹œê° ì •ë³´ (ì˜ˆ: ê·¸ë˜í”„, ì°¨íŠ¸, í™”í•™ êµ¬ì¡°ì‹)ì— íŠ¹í™”ëœ reward functionì„ ì„¤ê³„í•˜ì—¬ SAYO ëª¨ë¸ì„ ê°œì„ í•˜ê³ , scientific document understanding taskì— ì ìš©í•´ë´…ë‹ˆë‹¤.
2. **Vision Encoderì™€ SAYOì˜ ê²°í•©:** í˜„ì¬ ì—°êµ¬ ì¤‘ì¸ vision encoderì™€ SAYO ëª¨ë¸ì„ ê²°í•©í•˜ì—¬, vision encoderì—ì„œ ì¶”ì¶œëœ featureë¥¼ SAYOë¥¼ í†µí•´ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤.
3. **Attention Visualization ë° í•´ì„:** SAYO ëª¨ë¸ì˜ attention mapì„ ì‹œê°í™”í•˜ê³  ë¶„ì„í•˜ì—¬, ëª¨ë¸ì´ ì–´ë–¤ ë¶€ë¶„ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ë³´ê³  ì¶”ë¡ í•˜ëŠ”ì§€ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ reasoning processë¥¼ ì´í•´í•©ë‹ˆë‹¤.
4. **Reward Functionì˜ ë‹¤ì–‘ì„± ì—°êµ¬:** ë‹¤ì–‘í•œ reward functionì„ ì„¤ê³„í•˜ê³  ë¹„êµ ë¶„ì„í•˜ì—¬, visual attention í•™ìŠµì— ê°€ì¥ íš¨ê³¼ì ì¸ reward functionì„ íƒìƒ‰í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Multimodal Large Language Models (MLLMs)
2. Visual Attention
3. Reinforcement Learning (RL)
4. Vision-Language Alignment
5. Multimodal Reasoning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
