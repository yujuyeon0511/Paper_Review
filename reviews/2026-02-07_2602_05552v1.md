---
title: "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator"
date: 2026-02-07
arxiv: "2602.05552v1"
category: "cs.RO"
model: "google/gemma-3-27b-it:free"
---

# VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Bessie Dominguez-Dager, Sergio Suescun-Ferrandiz, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla |
| **ë°œí‘œì¼** | 2026-02-05 |
| **arXiv** | [2602.05552v1](https://arxiv.org/pdf/2602.05552v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.RO |

---

## ğŸ“ ì´ˆë¡ (Abstract)

This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

---

## ğŸ” AI ë¶„ì„

## VLN-Pilot ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ GPSê°€ ì œí•œëœ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ë“œë¡  í•­ë²•ì„ ìœ„í•´ ëŒ€ê·œëª¨ Vision-and-Language Model (VLLM)ì„ ì¸ê°„ ì¡°ì¢…ì‚¬ì²˜ëŸ¼ í™œìš©í•˜ëŠ” VLN-Pilot í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ í•´ì„í•˜ê³  ì‹œê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“œë¡ ì˜ ê¶¤ì ì„ ê³„íš ë° ì‹¤í–‰í•˜ë©°, ê¸°ì¡´ì˜ ê·œì¹™ ê¸°ë°˜ ë˜ëŠ” ê¸°í•˜í•™ì  ê²½ë¡œ ê³„íš ë°©ì‹ê³¼ ë‹¬ë¦¬ ì–¸ì–´ ê¸°ë°˜ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ì™€ ì‹œê°ì  ì¸ì‹ì„ í†µí•©í•©ë‹ˆë‹¤. VLN-PilotëŠ” ê³µê°„ ê´€ê³„, ì¥ì• ë¬¼ íšŒí”¼, ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì— ëŒ€í•œ ë™ì  ë°˜ì‘ì„ ì¶”ë¡ í•˜ì—¬ ì™„ì „ ììœ¨ì ì¸ ëª…ë ¹ ì¶”ì¢…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì‹¤ë‚´ UAV ì œì–´ì˜ í™•ì¥ì„±, ì‚¬ìš©ì ì¹œí™”ì„±, ì•ˆì „ì„±ì„ í–¥ìƒì‹œí‚¬ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
VLN-PilotëŠ” ê¸°ì¡´ ë“œë¡  í•­ë²• ì—°êµ¬ì™€ ì°¨ë³„í™”ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒˆë¡œìš´ ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

*   **VLLM ê¸°ë°˜ ììœ¨ í•­ë²•:** ê¸°ì¡´ì˜ ë“œë¡  í•­ë²• ì‹œìŠ¤í…œì´ ê·œì¹™ ê¸°ë°˜ ë˜ëŠ” ê¸°í•˜í•™ì  ê²½ë¡œ ê³„íšì— ì˜ì¡´í•˜ëŠ” ë°˜ë©´, ë³¸ ë…¼ë¬¸ì€ VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ ì§ì ‘ í•´ì„í•˜ê³  ì‹¤í–‰í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.
*   **ì–¸ì–´ ê¸°ë°˜ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ì™€ ì‹œê°ì  ì¸ì‹ í†µí•©:** ì–¸ì–´ ëª…ë ¹ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ì™€ ì‹œê° ì •ë³´ë¥¼ ìœµí•©í•˜ì—¬ ìƒí™© ì¸ì§€ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë³µì¡í•œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ë³´ë‹¤ ìœ ì—°í•˜ê³  ì§€ëŠ¥ì ì¸ í•­ë²•ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
*   **ì‹¤ë‚´ í™˜ê²½ì— íŠ¹í™”ëœ ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬:** ë³µì¡í•œ ì‹¤ë‚´ í™˜ê²½ì—ì„œì˜ ë“œë¡  í•­ë²• ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ photorealistic ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í•˜ì—¬, VLLM ê¸°ë°˜ ë“œë¡  í•­ë²• ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
*   **ë†’ì€ ì„±ê³µë¥ :** ë³µì¡í•œ ëª…ë ¹ ì¶”ì¢… ì‘ì—…ì—ì„œ ë†’ì€ ì„±ê³µë¥ ì„ ë³´ì—¬ì£¼ë©°, VLLM ê¸°ë°˜ ë“œë¡  í•­ë²• ì‹œìŠ¤í…œì˜ ì‹¤ìš©ì ì¸ ê°€ëŠ¥ì„±ì„ ì…ì¦í•©ë‹ˆë‹¤.
*   **ìœ ì—°ì„± ë° ì ì‘ì„±:** ìì—°ì–´ ëª…ë ¹ì„ í†µí•´ ë“œë¡ ì˜ í–‰ë™ì„ ì œì–´í•  ìˆ˜ ìˆì–´, ë‹¤ì–‘í•œ ì‘ì—… í™˜ê²½ê³¼ ìš”êµ¬ ì‚¬í•­ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **í™•ì¥ì„± ë° ì‚¬ìš©ì ì¹œí™”ì„±:** ì›ê²© ì¡°ì¢…ì‚¬ì˜ ë¶€ë‹´ì„ ì¤„ì´ê³ , ë“œë¡  ìš´ì˜ì˜ í™•ì¥ì„±ê³¼ ì‚¬ìš©ì ì¹œí™”ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
*   **ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì˜ì¡´ì„±:** ì‹¤í—˜ ê²°ê³¼ê°€ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ì–»ì–´ì§„ ê²ƒìœ¼ë¡œ, ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì˜ ë³µì¡ì„±ê³¼ ë¶ˆí™•ì‹¤ì„±ì€ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ì™„ë²½í•˜ê²Œ ì¬í˜„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
*   **VLLMì˜ í•œê³„:** VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì€ ì™„ë²½í•˜ì§€ ì•Šìœ¼ë©°, ë³µì¡í•˜ê±°ë‚˜ ëª¨í˜¸í•œ ëª…ë ¹ì— ëŒ€í•œ ì˜¤í•´ì„ ê°€ëŠ¥ì„±ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë˜í•œ, VLLMì˜ ê³„ì‚° ë¹„ìš©ì´ ë†’ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
*   **ì•ˆì „ì„± ê²€ì¦ ë¶€ì¡±:** ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì— ëŒ€í•œ ë“œë¡ ì˜ ë°˜ì‘ì— ëŒ€í•œ ì•ˆì „ì„± ê²€ì¦ì´ ì¶©ë¶„íˆ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œ ë“œë¡ ì„ ìš´ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” ì•ˆì „ì„± í™•ë³´ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì €ì˜ ì—°êµ¬ ë¶„ì•¼ì¸ Scientific MLLM, Reasoning MLLM, Vision-Language Alignmentì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ìŒê³¼ ê°™ì€ ì¸¡ë©´ì—ì„œ ì—°ê´€ì„±ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

*   **Vision-Language Alignment:** VLN-PilotëŠ” ì‹œê° ì •ë³´ì™€ ì–¸ì–´ ëª…ë ¹ ê°„ì˜ ì •ë ¬(alignment)ì„ í†µí•´ ë“œë¡  í•­ë²•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì €ì˜ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ ì •ë ¬ ê¸°ìˆ ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, VLN-Pilotì˜ ì—°êµ¬ ê²°ê³¼ë¥¼ í†µí•´ ì •ë ¬ ê¸°ìˆ ì˜ ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **Multimodal Reasoning:** VLN-PilotëŠ” VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ë“œë¡  í•­ë²•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì €ì˜ ì—°êµ¬ëŠ” MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, VLN-Pilotì˜ ì—°êµ¬ ê²°ê³¼ë¥¼ í†µí•´ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì˜ ì¤‘ìš”ì„±ì„ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **Scientific ë„ë©”ì¸ ì ìš© ê°€ëŠ¥ì„±:** VLN-Pilotì˜ ê¸°ìˆ ì€ ê³¼í•™ì  ë„ë©”ì¸, ì˜ˆë¥¼ ë“¤ì–´ ì—°êµ¬ ì‹œì„¤ ë‚´ë¶€ ê²€ì‚¬, ìœ„í—˜ ì§€ì—­ íƒì‚¬ ë“±ì— ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
*   **ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ê²€ì¦:** VLN-Pilotë¥¼ ì‹¤ì œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ê³ , ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì˜ ë…¸ì´ì¦ˆì™€ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•˜ì—¬ VLLMì˜ robustnessë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
*   **Scientific ë„ë©”ì¸ íŠ¹í™”ëœ VLLM í•™ìŠµ:** ê³¼í•™ì  ë„ë©”ì¸ì— íŠ¹í™”ëœ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ VLLMì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—°êµ¬ ì‹œì„¤ ë‚´ë¶€ êµ¬ì¡°, ì¥ë¹„ ì •ë³´, ì•ˆì „ ê·œì • ë“±ì„ í•™ìŠµì‹œì¼œ ë“œë¡ ì˜ í•­ë²• ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
*   **VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ:** VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ìƒˆë¡œìš´ í•™ìŠµ ë°©ë²•ë¡ ì„ ê°œë°œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê°•í™” í•™ìŠµì„ í†µí•´ ë“œë¡ ì˜ í•­ë²• ì„±ëŠ¥ì„ ìµœì í™”í•˜ê±°ë‚˜, few-shot learningì„ í†µí•´ ìƒˆë¡œìš´ ì‘ì—…ì— ëŒ€í•œ ì ì‘ë ¥ì„ ë†’ì…ë‹ˆë‹¤.
*   **ì•ˆì „ì„± ê°•í™”:** ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì— ëŒ€í•œ ë“œë¡ ì˜ ë°˜ì‘ì„ ì˜ˆì¸¡í•˜ê³ , ì•ˆì „ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ë©”ì»¤ë‹ˆì¦˜ì„ ê°œë°œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¶©ëŒ íšŒí”¼ ì•Œê³ ë¦¬ì¦˜ì„ ê°œì„ í•˜ê±°ë‚˜, ë¹„ìƒ ì •ì§€ ê¸°ëŠ¥ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
*   Vision-Language Model (VLLM)
*   Indoor Drone Navigation
*   Multimodal Reasoning
*   Instruction Following
*   Autonomous Agent

---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
