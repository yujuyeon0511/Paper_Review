---
title: "FewMMBench: A Benchmark for Multimodal Few-Shot Learning"
date: 2026-02-26
arxiv: "2602.21854v1"
category: "cs.CL"
model: "google/gemma-3-27b-it:free"
---

# FewMMBench: A Benchmark for Multimodal Few-Shot Learning

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Mustafa Dogan, Ilker Kesen, Iacer Calixto, Aykut Erdem, Erkut Erdem |
| **ë°œí‘œì¼** | 2026-02-25 |
| **arXiv** | [2602.21854v1](https://arxiv.org/pdf/2602.21854v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CL |

---

## ğŸ“ ì´ˆë¡ (Abstract)

As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench

---

## ğŸ” AI ë¶„ì„

## ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ Multimodal Large Language Model (MLLM)ì˜ Few-Shot Learning ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ FewMMBenchë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. FewMMBenchëŠ” ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ê³¼ì œë¥¼ í¬í•¨í•˜ë©°, In-Context Learning (ICL) ë° Chain-of-Thought (CoT) í”„ë¡¬í”„íŒ… ì „ëµ í•˜ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. 26ê°œì˜ ì˜¤í”ˆ ì›¨ì´íŠ¸ MLLMì„ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜í•œ ê²°ê³¼, instruction-tuned ëª¨ë¸ì€ zero-shot ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ì§€ë§Œ, ì¶”ê°€ì ì¸ ë°ëª¨ë‚˜ CoT ì¶”ë¡ ì„ í™œìš©í•´ë„ ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ì €í•˜ë˜ëŠ” ê²½í–¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” MLLMì˜ few-shot learning ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
FewMMBenchëŠ” ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ê°€ ì£¼ë¡œ zero-shot ì„±ëŠ¥ì— ì§‘ì¤‘í–ˆë˜ ê²ƒê³¼ ë‹¬ë¦¬, **MLLMì˜ few-shot learning ëŠ¥ë ¥, íŠ¹íˆ ICLê³¼ CoT í”„ë¡¬í”„íŒ… ì „ëµ í•˜ì—ì„œì˜ ì„±ëŠ¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ í‰ê°€**í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ë¼ëŠ” ì ì´ ê°€ì¥ í° ì°¨ë³„ì ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ê³¼ì œë¥¼ í¬í•¨í•˜ì—¬ MLLMì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í­ë„“ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬:** ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ê³¼ì œë¥¼ í¬í•¨í•˜ì—¬ MLLMì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **Few-Shot Learning ì§‘ì¤‘:** ICL ë° CoT í”„ë¡¬í”„íŒ… ì „ëµì„ í™œìš©í•˜ì—¬ MLLMì˜ few-shot learning ëŠ¥ë ¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
3. **ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ í‰ê°€:** 26ê°œì˜ ì˜¤í”ˆ ì›¨ì´íŠ¸ MLLMì„ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜í•˜ì—¬ ì—°êµ¬ ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ë†’ì´ê³ , ì»¤ë®¤ë‹ˆí‹°ì— ê¸°ì—¬í•©ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **í•œêµ­ì–´ ë°ì´í„° ë¶€ì¡±:** ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì´ ì˜ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ í•œêµ­ì–´ MLLMì˜ ì„±ëŠ¥ í‰ê°€ì—ëŠ” ì§ì ‘ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
2. **Task ë‹¤ì–‘ì„± ì œí•œ:** ë¬¸ì„œ/ì°¨íŠ¸/OCR/í…Œì´ë¸” VQAì™€ ê°™ì€ íŠ¹ì • ë¶„ì•¼ì— ëŒ€í•œ ê³¼ì œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì˜ì¡´ì„±:** ICL ë° CoT í”„ë¡¬í”„íŒ… ì „ëµì˜ ì„±ëŠ¥ì€ í”„ë¡¬í”„íŠ¸ì˜ í’ˆì§ˆì— í¬ê²Œ ì˜ì¡´í•˜ë©°, ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì œê°€ ì§„í–‰í•˜ê³  ìˆëŠ” **Vision encoderì™€ Text LLM ê°„ì˜ ì •ë ¬(alignment) ì—°êµ¬**ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤. Few-shot learning ì„±ëŠ¥ ì €í•˜ í˜„ìƒì€ ëª¨ë¸ ê°„ì˜ ì •ë ¬ì´ ì¶©ë¶„íˆ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” alignment ì—°êµ¬ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ë˜í•œ, **í•œêµ­ì–´ Multimodal LLM ê°œë°œ ë° í‰ê°€**ë¥¼ ìœ„í•´ FewMMBenchì™€ ìœ ì‚¬í•œ í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í•˜ëŠ” ì•„ì´ë””ì–´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, **ë¬¸ì„œ/ì°¨íŠ¸/OCR/í…Œì´ë¸” ì´í•´ íŠ¹í™” MLLM ì—°êµ¬**ì— í•„ìš”í•œ few-shot learning ë°ì´í„°ì…‹ êµ¬ì¶•ì— ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **í•œêµ­ì–´ Few-Shot MLLM ë²¤ì¹˜ë§ˆí¬ êµ¬ì¶•:** FewMMBenchë¥¼ ì°¸ê³ í•˜ì—¬ í•œêµ­ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±ëœ few-shot MLLM ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í•˜ê³ , í•œêµ­ì–´ MLLMì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
2. **Alignment ê¸°ë²•ê³¼ Few-Shot Learning ì„±ëŠ¥ì˜ ìƒê´€ê´€ê³„ ë¶„ì„:** Vision encoderì™€ Text LLM ê°„ì˜ alignment ê¸°ë²• (ì˜ˆ: projection layer, adapter)ì„ ì ìš©í•œ í›„, FewMMBenchì™€ ìœ ì‚¬í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ few-shot learning ì„±ëŠ¥ ë³€í™”ë¥¼ ì¸¡ì •í•˜ì—¬ alignment ê¸°ë²•ê³¼ few-shot learning ì„±ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
3. **DocVQA/ChartQA/TableVQA íŠ¹í™” Few-Shot Learning ì—°êµ¬:** ë¬¸ì„œ, ì°¨íŠ¸, í…Œì´ë¸” ì´í•´ì— íŠ¹í™”ëœ few-shot learning ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ , í•´ë‹¹ ë°ì´í„°ì…‹ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. íŠ¹íˆ, CoT í”„ë¡¬í”„íŒ… ì „ëµì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Multimodal Large Language Models (MLLM)
2. Few-Shot Learning
3. In-Context Learning (ICL)
4. Chain-of-Thought (CoT)
5. Vision-Language Alignment


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
