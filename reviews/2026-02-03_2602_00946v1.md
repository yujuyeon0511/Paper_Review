---
title: "ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Langu"
date: 2026-02-03
arxiv: "2602.00946v1"
category: "cs.CV"
model: "google/gemma-3-4b-it:free"
---

# ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Dhruv Parikh, Haoyang Fan, Rajgopal Kannan, Viktor Prasanna |
| **ë°œí‘œì¼** | 2026-02-01 |
| **arXiv** | [2602.00946v1](https://arxiv.org/pdf/2602.00946v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CV |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ConsensusDropëŠ” VLMsì˜ íš¨ìœ¨ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¹„ì „ ì¸ì½”ë”ì˜ saliencyì™€ LLMì˜ cross-attention ì •ë³´ë¥¼ ê²°í•©í•˜ëŠ” ìƒˆë¡œìš´ pruning ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ë‹¨ì¼ ì‹ í˜¸ì— ì˜ì¡´í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë‘ ì‹ í˜¸ë¥¼ ì¡°í™”ì‹œì¼œ ê°€ì¥ ì •ë³´ëŸ‰ì´ ë†’ì€ í† í°ì„ ìœ ì§€í•˜ë©´ì„œ ë‚˜ë¨¸ì§€ëŠ” ì¸ì½”ë”ë¥¼ í†µí•´ ë³‘í•©í•˜ëŠ” â€˜í•©ì˜ ê¸°ë°˜ ìˆœìœ„ ê²°ì •â€™ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. LLaVA-1.5, Video-LLaVA ë“± ë‹¤ì–‘í•œ VLMsì—ì„œ ê¸°ì¡´ pruning ë°©ë²•ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°, í† í° ì˜ˆì‚°ì´ ë™ì¼í•˜ë©´ì„œë„ ì •í™•ë„ì™€ íš¨ìœ¨ì„± Pareto Frontierë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ë˜í•œ, TTFTì™€ KV cache footprintë¥¼ ì¤„ì—¬ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ë”ìš± ë†’ì…ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ConsensusDropì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ ë¹„ì „ ì¸ì½”ë”ì˜ saliencyì™€ LLMì˜ cross-attention ì •ë³´ë¥¼ ë‹¨ìˆœíˆ ê²°í•©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, â€˜í•©ì˜ ê¸°ë°˜ ìˆœìœ„ ê²°ì •â€™ì´ë¼ëŠ” ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë‘ ì‹ í˜¸ì˜ ì¥ì ì„ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ saliencyë‚˜ cross-attention ì¤‘ í•˜ë‚˜ì˜ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆì—ˆë˜ ë°˜ë©´, ConsensusDropëŠ” ë‘ ì‹ í˜¸ë¥¼ ê· í˜• ìˆê²Œ ì¡°í™”ì‹œì¼œ í† í° ì„ íƒì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. ë˜í•œ, training-free ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ê¸°ì¡´ pruning ë°©ë²•ì˜ ì¶”ê°€ì ì¸ í•™ìŠµ ê³¼ì • ì—†ì´ ì ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ë„ ì¤‘ìš”í•œ ì°¨ë³„ì ì…ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **íš¨ìœ¨ì ì¸ í† í° ì„ íƒ:** ë¹„ì „ ì¸ì½”ë” saliencyì™€ LLM cross-attention ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê³ , í† í° ì„ íƒì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
2. **ë†’ì€ ì •í™•ë„ ìœ ì§€:** í† í° ê°ì†Œì—ë„ ë¶ˆêµ¬í•˜ê³ , ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, Pareto Frontierë¥¼ ê°œì„ í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì •í™•ë„ ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤.
3. **Training-free ë°©ì‹:** ì¶”ê°€ì ì¸ í•™ìŠµ ê³¼ì • ì—†ì´ ê¸°ì¡´ VLMsì— ì ìš© ê°€ëŠ¥í•˜ì—¬, ì—°êµ¬ ë° ê°œë°œì˜ ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **Saliency ì‹ í˜¸ ì˜ì¡´ì„±:** ë¹„ì „ ì¸ì½”ë” saliency ì‹ í˜¸ì˜ í’ˆì§ˆì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ì¢Œìš°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. saliency ì‹ í˜¸ì˜ ì •í™•ë„ê°€ ë‚®ì„ ê²½ìš°, í† í° ì„ íƒì˜ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **LLM êµ¬ì¡° ì˜ì¡´ì„±:** LLMì˜ êµ¬ì¡°ì— ë”°ë¼ cross-attention ì •ë³´ì˜ í™œìš©ë„ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • LLM êµ¬ì¡°ì—ì„œëŠ” ConsensusDropì˜ íš¨ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ ë³µì¡ì„±:** â€˜í•©ì˜ ê¸°ë°˜ ìˆœìœ„ ê²°ì •â€™ ì•Œê³ ë¦¬ì¦˜ì˜ êµ¬í˜„ ë° ìµœì í™”ê°€ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë©°, ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ConsensusDropëŠ” í˜„ì¬ ì§„í–‰ ì¤‘ì¸ Vision Encoderì™€ Text LLM ê°„ì˜ ì •ë ¬ ì—°êµ¬ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ, LLMì˜ cross-attention ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¹„ì „ í† í°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ë°©ë²•ì€ í˜„ì¬ ì—°êµ¬ ë°©í–¥ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤. ë˜í•œ, multimodal reasoning ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•´ MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•˜ëŠ” ì—°êµ¬ì—ë„ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ConsensusDropì˜ training-free ë°©ì‹ì€ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í˜„ì¬ ì—°êµ¬ì—ì„œ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•˜ëŠ” ìš”ì†Œì…ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **Saliency ì‹ í˜¸ ê°œì„ :** ë‹¤ì–‘í•œ saliency ì‹ í˜¸(ì˜ˆ: attention map, gradient)ë¥¼ ê²°í•©í•˜ì—¬ saliency ì‹ í˜¸ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **LLM êµ¬ì¡°ë³„ ìµœì í™”:** ë‹¤ì–‘í•œ LLM êµ¬ì¡°ì— ëŒ€í•œ ConsensusDropì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•˜ê³ , ê° êµ¬ì¡°ì— ìµœì í™”ëœ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ë‹¤ë¥¸ pruning ê¸°ë²•ê³¼ì˜ ê²°í•©:** ConsensusDropì™€ ë‹¤ë¥¸ pruning ê¸°ë²•(ì˜ˆ: magnitude pruning, weight quantization)ì„ ê²°í•©í•˜ì—¬ ë”ìš± íš¨ìœ¨ì ì¸ ëª¨ë¸ ì••ì¶• ë°©ë²•ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **Scientific ë„ë©”ì¸ ì ìš©:** Scientific ë¬¸ì„œ ì´í•´ë¥¼ ìœ„í•œ MLLM ê°œë°œì— ConsensusDropë¥¼ ì ìš©í•˜ì—¬, ë¬¸ì„œì˜ í•µì‹¬ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì¶”ë¡ í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. **Visual Instruction Tuning í™œìš©:** ConsensusDropë¥¼ Visual Instruction Tuning ê³¼ì •ì— ì ìš©í•˜ì—¬, ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Vision-Language Models (VLMs)
2. Token Pruning
3. Saliency
4. Cross-Attention
5. Multimodal Reasoning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
