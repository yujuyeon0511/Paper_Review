---
title: "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Un"
date: 2026-02-11
arxiv: "2503.23733"
category: "Computer Vision and Pattern Recognition"
model: "google/gemma-3-4b-it:free"
---

# AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Yiyang Du, Xiaochen Wang, Chi Chen, Jiabo Ye, Yiru Wang... |
| **ë°œí‘œì¼** | 2025-03-31 |
| **arXiv** | [2503.23733](https://arxiv.org/pdf/2503.23733) |
| **ì¹´í…Œê³ ë¦¬** | Computer Vision and Pattern Recognition |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS 1, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks. 2

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: AdaMMS - ëª¨ë¸ ë³‘í•©ì„ í†µí•œ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ LLM ì„±ëŠ¥ í–¥ìƒ

### ë…¼ë¬¸ ìš”ì•½
AdaMMSëŠ” ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ LLM(MLLM)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ ë³‘í•© ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ê¸°ì¡´ ë°©ë²•ë“¤ì´ ì£¼ë¡œ ë™ì¼ ì•„í‚¤í…ì²˜ ëª¨ë¸ ë³‘í•©ì— ì§‘ì¤‘í–ˆë˜ ë°˜ë©´, AdaMMSëŠ” ëª¨ë¸ ê°„ì˜ ì•„í‚¤í…ì²˜ ì°¨ì´ë¥¼ ê·¹ë³µí•˜ê³ , unsupervised ë°©ì‹ìœ¼ë¡œ ë³‘í•© íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ì—¬ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. íŠ¹íˆ í•œêµ­ì–´ MLLM ê°œë°œ ë° í‰ê°€ì— íŠ¹í™”ëœ MLLMì— ëŒ€í•œ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë°©ë²•ë¡  (Methodology)
AdaMMSëŠ” ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ëª¨ë¸ ë³‘í•© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

1.  **Mapping (ë§¤í•‘):** ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ì˜ ëª¨ë¸ ê°„ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë§¤í•‘í•˜ì—¬ ë³‘í•© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤. íŠ¹íˆ, ì„œë¡œ ë‹¤ë¥¸ transformer block duplicationì„ ê°€ì§„ ëª¨ë¸ì˜ ê²½ìš°, í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ë§¤í•‘í•©ë‹ˆë‹¤.
2.  **Merging (ë³‘í•©):** ë§¤í•‘ëœ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ í˜• ë³´ê°„ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤. ì´ë•Œ, ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
3.  **Searching (íƒìƒ‰):** unsupervised ë°©ì‹ìœ¼ë¡œ ë³‘í•© íŒŒë¼ë¯¸í„°(interpolation coefficient)ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤. ëª¨ë¸ ì„±ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ generation consistencyë¥¼ ì¸¡ì •í•˜ì—¬, ì´ë¥¼ í†µí•´ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
*   **Parameter Mapping:** ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ ëª¨ë¸ ê°„ì˜ íŒŒë¼ë¯¸í„° ì¼ì¹˜
*   **Adaptive Linear Interpolation:** ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³‘í•© ì‹œ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •
*   **Unsupervised Hyperparameter Selection:** generation consistencyë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë³‘í•© íŒŒë¼ë¯¸í„° íƒìƒ‰

backbone ëª¨ë¸ë¡œëŠ” Qwen2, LLaMA ë“±ì´ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, vision encoderëŠ” LLaVA ë“±ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ì„ í™œìš©í–ˆìœ¼ë©°, multi-stage training ì „ëµì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

### ì‹¤í—˜ ë° ê²°ê³¼ (Experiments & Results)
AdaMMSëŠ” ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ LLM ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤í—˜ë˜ì—ˆìŠµë‹ˆë‹¤.

*   **ë²¤ì¹˜ë§ˆí¬:** MME, SEED-Bench, OCRBench, TextVQA, OKVQA, GQA, VizWizval ë“± ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
*   **ë¹„êµ ëŒ€ìƒ ëª¨ë¸:** Task Arithmetic, Ties-Merging, DARE-Linear, DARE-Ties, MetaGPT ë“± ê¸°ì¡´ ëª¨ë¸ ë³‘í•© ë°©ë²•ë“¤ê³¼ ë¹„êµí–ˆìŠµë‹ˆë‹¤.
*   **ì£¼ìš” ê²°ê³¼:**
    *   ë‹¤ì–‘í•œ ëª¨ë¸ ì¡°í•©ì—ì„œ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
    *   íŠ¹íˆ, í•œêµ­ì–´ MLLM ê°œë°œ ë° í‰ê°€ì— íŠ¹í™”ëœ MLLMì—ì„œ ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ê°€ ì»¸ìŠµë‹ˆë‹¤.
    *   unsupervised hyperparameter selection ë°©ë²•ì€ labeled data ì—†ì´ë„ íš¨ê³¼ì ìœ¼ë¡œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.
    *   Ablation study ê²°ê³¼, mapping ë‹¨ê³„ì˜ ì¤‘ìš”ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
*   **SOTA ë‹¬ì„± ì—¬ë¶€:** ì¼ë¶€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

### ìƒˆë¡œìš´ ì  (Novelty)
AdaMMSì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

*   **Heterogeneous MLLM ë³‘í•©:** ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ MLLMì„ ë³‘í•©í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤.
*   **Unsupervised Hyperparameter Selection:** labeled data ì—†ì´ unsupervised ë°©ì‹ìœ¼ë¡œ ìµœì ì˜ ë³‘í•© íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.
*   **Generation Consistency ê¸°ë°˜ ì„±ëŠ¥ ì˜ˆì¸¡:** ëª¨ë¸ ì„±ëŠ¥ì„ generation consistencyë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì—¬, ë³‘í•© íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

### ê°•ì  (Strengths)
*   **ë‹¤ì–‘í•œ MLLM ì§€ì›:** ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ë‹¤ì–‘í•œ MLLMì„ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **Unsupervised ë°©ì‹:** labeled data ì—†ì´ë„ íš¨ê³¼ì ì¸ ëª¨ë¸ ë³‘í•©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
*   **ë†’ì€ ì„±ëŠ¥:** ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

### ì•½ì /í•œê³„ì  (Limitations)
*   **Mapping ë‹¨ê³„ì˜ ë³µì¡ì„±:** ëª¨ë¸ ê°„ì˜ íŒŒë¼ë¯¸í„° mapping ê³¼ì •ì´ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **Generation Consistency ì¸¡ì •ì˜ ì–´ë ¤ì›€:** generation consistencyë¥¼ ì •í™•í•˜ê²Œ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì˜ì¡´ì„±:** ì‹¤í—˜ ê²°ê³¼ëŠ” ì‚¬ìš©ëœ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
AdaMMSëŠ” ì‚¬ìš©ìì˜ MLLM merging, Korean MLLM, vision-text alignment, document/chart/OCR/table VQA ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë©ë‹ˆë‹¤. íŠ¹íˆ, AdaMMSëŠ” ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ MLLMì„ ë³‘í•©í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, ì´ëŠ” document/chart/OCR/table VQAì™€ ê°™ì´ ë‹¤ì–‘í•œ modalityë¥¼ í™œìš©í•˜ëŠ” MLLM ê°œë°œì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, Korean MLLM ê°œë°œì— íŠ¹í™”ëœ MLLMì— ëŒ€í•œ ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ëŠ” í•œêµ­ì–´ ê¸°ë°˜ì˜ VQA ì‹œìŠ¤í…œ ê°œë°œì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1.  **Adaptive Mapping ì „ëµ ê°œë°œ:** ëª¨ë¸ ê°„ì˜ íŒŒë¼ë¯¸í„° mapping ê³¼ì •ì„ ìë™í™”í•˜ê³ , mappingì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” adaptive mapping ì „ëµì„ ê°œë°œí•©ë‹ˆë‹¤.
2.  **Generation Consistency ì¸¡ì • ë°©ë²• ê°œì„ :** generation consistencyë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ì–‘í•œ generation metricsë¥¼ í™œìš©í•˜ê±°ë‚˜, adversarial trainingì„ í†µí•´ generation consistencyë¥¼ ê°•í™”í•˜ëŠ” ë°©ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3.  **Domain-Specific Fine-tuning:** AdaMMSë¥¼ í†µí•´ ë³‘í•©ëœ MLLMì„ íŠ¹ì • domainì— ë§ê²Œ fine-tuningí•˜ì—¬ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ë£Œ ë¶„ì•¼ì˜ MLLMì„ ê°œë°œí•˜ê¸° ìœ„í•´ ì˜ë£Œ ê´€ë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tuningí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ í‚¤ì›Œë“œ
MLLM, ëª¨ë¸ ë³‘í•©, unsupervised learning, hyperparameter optimization, vision-language alignment, Korean MLLM, document VQA, chart VQA, OCR multimodal, table VQA


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
