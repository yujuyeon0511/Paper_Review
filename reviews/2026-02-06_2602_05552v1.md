---
title: "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator"
date: 2026-02-06
arxiv: "2602.05552v1"
category: "cs.RO"
model: "google/gemma-3-27b-it:free"
---

# VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Bessie Dominguez-Dager, Sergio Suescun-Ferrandiz, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla |
| **ë°œí‘œì¼** | 2026-02-05 |
| **arXiv** | [2602.05552v1](https://arxiv.org/pdf/2602.05552v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.RO |

---

## ğŸ“ ì´ˆë¡ (Abstract)

This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

---

## ğŸ” AI ë¶„ì„

## VLN-Pilot ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ GPSê°€ ì œí•œëœ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ë“œë¡  í•­ë²•ì„ ìœ„í•´ ëŒ€ê·œëª¨ Vision-and-Language ëª¨ë¸(VLLM)ì„ ì¸ê°„ ì¡°ì¢…ì‚¬ì²˜ëŸ¼ í™œìš©í•˜ëŠ” VLN-Pilot í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ í•´ì„í•˜ê³  ì‹œê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“œë¡ ì˜ ê¶¤ì ì„ ê³„íš ë° ì‹¤í–‰í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ê·œì¹™ ê¸°ë°˜ ë˜ëŠ” ê¸°í•˜í•™ì  ê²½ë¡œ ê³„íš ë°©ì‹ê³¼ ë‹¬ë¦¬, VLN-Pilotì€ ì–¸ì–´ ê¸°ë°˜ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ì™€ ì‹œê°ì  ì¸ì‹ì„ í†µí•©í•˜ì—¬ ìƒí™© ì¸ì§€ì ì¸ ê³ ìˆ˜ì¤€ ë¹„í–‰ í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, VLN-PilotëŠ” ë³µì¡í•œ ëª…ë ¹ ì¶”ì¢… ì‘ì—…ì—ì„œ ë†’ì€ ì„±ê³µë¥ ì„ ë³´ì´ë©°, ì‹¤ë‚´ UAV ì œì–´ì˜ í™•ì¥ì„±, ì‚¬ìš©ì ì¹œí™”ì„±, ì•ˆì „ì„±ì„ í–¥ìƒì‹œí‚¬ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
VLN-PilotëŠ” ê¸°ì¡´ ë“œë¡  í•­ë²• ì—°êµ¬ì™€ ì°¨ë³„í™”ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒˆë¡œìš´ ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

*   **VLLM ê¸°ë°˜ ììœ¨ í•­ë²•:** ê¸°ì¡´ì˜ ë“œë¡  í•­ë²• ì‹œìŠ¤í…œì´ ê·œì¹™ ê¸°ë°˜ ë˜ëŠ” ê¸°í•˜í•™ì  ì•Œê³ ë¦¬ì¦˜ì— ì˜ì¡´í•˜ëŠ” ë°˜ë©´, VLN-PilotëŠ” VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ ì§ì ‘ í•´ì„í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
*   **ì–¸ì–´ ê¸°ë°˜ì˜ ê³ ìˆ˜ì¤€ ë¹„í–‰ í–‰ë™:** VLLMì„ í†µí•´ ê³µê°„ ê´€ê³„, ì¥ì• ë¬¼ íšŒí”¼, ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì— ëŒ€í•œ ë™ì  ë°˜ì‘ê³¼ ê°™ì€ ê³ ìˆ˜ì¤€ì˜ ë¹„í–‰ í–‰ë™ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
*   **ìµœì†Œí•œì˜ task-specific engineering:** ê¸°ì¡´ ë°©ì‹ ëŒ€ë¹„ task-specificí•œ ì—”ì§€ë‹ˆì–´ë§ ë…¸ë ¥ì„ ìµœì†Œí™”í•˜ê³ , VLLMì˜ ì¼ë°˜ì ì¸ ì´í•´ ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ í™˜ê²½ì— ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1.  **ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤:** ìì—°ì–´ ëª…ë ¹ì„ í†µí•´ ë“œë¡ ì„ ì œì–´í•  ìˆ˜ ìˆì–´ ì‚¬ìš© í¸ì˜ì„±ì´ ë†’ê³ , ì „ë¬¸ì ì¸ ì§€ì‹ ì—†ì´ë„ ë“œë¡ ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2.  **ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥:** VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ í™˜ê²½ì—ì„œ ìƒí™© ì¸ì§€ì ì¸ ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3.  **í™•ì¥ì„± ë° ìœ ì—°ì„±:** ë‹¤ì–‘í•œ ì‹¤ë‚´ í™˜ê²½ ë° ì‘ì—…ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ìƒˆë¡œìš´ ëª…ë ¹ì´ë‚˜ í™˜ê²½ì— ëŒ€í•œ ì ì‘ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1.  **VLLMì˜ ì‹ ë¢°ì„±:** VLLMì˜ ì¶”ë¡  ì˜¤ë¥˜ëŠ” ë“œë¡ ì˜ ì˜¤ì‘ë™ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ì•ˆì „ì´ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œëŠ” ì‹ ë¢°ì„± í™•ë³´ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.
2.  **ì‹¤ì‹œê°„ ì„±ëŠ¥:** VLLMì˜ ì¶”ë¡  ì†ë„ê°€ ì‹¤ì‹œê°„ ë“œë¡  ì œì–´ì— í•„ìš”í•œ ìˆ˜ì¤€ì¸ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ë³µì¡í•œ í™˜ê²½ì—ì„œëŠ” ì¶”ë¡  ì‹œê°„ì´ ê¸¸ì–´ì ¸ ì„±ëŠ¥ ì €í•˜ë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3.  **ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì˜ì¡´ì„±:** í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ê²€ì¦ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì˜ ë…¸ì´ì¦ˆ, ì¡°ëª… ë³€í™”, ì„¼ì„œ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì €ì˜ ì—°êµ¬ ë¶„ì•¼ì¸ Scientific MLLM, Reasoning MLLM, Vision-Language Alignmentì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ìŒê³¼ ê°™ì€ ì¸¡ë©´ì—ì„œ ì—°ê´€ì„±ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

*   **Vision-Language Alignment:** VLN-PilotëŠ” ì‹œê° ì •ë³´ì™€ ì–¸ì–´ ëª…ë ¹ ê°„ì˜ ì •ë ¬ì„ í†µí•´ ë“œë¡ ì„ ì œì–´í•©ë‹ˆë‹¤. ì´ëŠ” ì €ì˜ ì—°êµ¬ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ì£¼ì œì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.
*   **Multimodal Reasoning:** VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì€ ë“œë¡ ì˜ ììœ¨ í•­ë²•ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì €ì˜ ì—°êµ¬ëŠ” MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, VLN-Pilotì˜ ì„±ê³µì€ ì´ëŸ¬í•œ ì—°êµ¬ì˜ ì¤‘ìš”ì„±ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤.
*   **Scientific ë„ë©”ì¸ ì ìš© ê°€ëŠ¥ì„±:** VLN-Pilotì˜ ê¸°ìˆ ì€ ê³¼í•™ì  ì—°êµ¬ ì‹œì„¤, í”ŒëœíŠ¸, ê±´ì„¤ í˜„ì¥ ë“± ë‹¤ì–‘í•œ Scientific ë„ë©”ì¸ì—ì„œ ë“œë¡ ì„ í™œìš©í•˜ì—¬ ê²€ì‚¬, ëª¨ë‹ˆí„°ë§, ë°ì´í„° ìˆ˜ì§‘ ë“±ì˜ ì‘ì—…ì„ ìë™í™”í•˜ëŠ” ë° ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1.  **Scientific ë„ë©”ì¸ íŠ¹í™” VLN-Pilot:** ê³¼í•™ ì‹œì„¤(ì˜ˆ: ì—°êµ¬ì‹¤, í”ŒëœíŠ¸)ì˜ êµ¬ì¡°ì  íŠ¹ì§•ê³¼ ì‘ì—… ìš”êµ¬ ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ VLN-Pilotë¥¼ ê°œì„ í•˜ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì¥ë¹„ì˜ ìœ„ì¹˜ë¥¼ ì¸ì‹í•˜ê³  ì ê²€í•˜ëŠ” ëª…ë ¹ì„ ìˆ˜í–‰í•˜ë„ë¡ VLLMì„ fine-tuningí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2.  **VLLMì˜ ì¶”ë¡  ì‹ ë¢°ë„ í–¥ìƒ:** VLLMì˜ ì¶”ë¡  ê²°ê³¼ì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ ì¸¡ì •í•˜ê³ , ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ìš”ì²­í•˜ê±°ë‚˜ ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3.  **ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ê²€ì¦:** VLN-Pilotë¥¼ ì‹¤ì œ ì‹¤ë‚´ í™˜ê²½ì— ì ìš©í•˜ê³ , ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„  ë°©ì•ˆì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì¡°ëª… ë³€í™”, ì„¼ì„œ ë…¸ì´ì¦ˆ, ì˜ˆìƒì¹˜ ëª»í•œ ì¥ì• ë¬¼ ë“±ì— ëŒ€í•œ ê°•ê±´ì„±ì„ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
4.  **Vision Encoder ê°œì„ ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ:** VLN-Pilotì— ì‚¬ìš©ë˜ëŠ” Vision Encoderì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ì—¬ ì‹œê° ì •ë³´ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê³ , VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1.  Vision-Language Model (VLLM)
2.  Autonomous Drone Navigation
3.  Multimodal Reasoning
4.  Indoor Localization
5.  Instruction Following

---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
