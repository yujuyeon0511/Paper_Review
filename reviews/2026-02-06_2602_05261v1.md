---
title: "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response"
date: 2026-02-06
arxiv: "2602.05261v1"
category: "cs.CL"
model: "google/gemma-3-4b-it:free"
---

# Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Fanfan Liu, Youyang Yin, Peng Shi, Siqi Yang, Zhixiong Zeng... |
| **ë°œí‘œì¼** | 2026-02-05 |
| **arXiv** | [2602.05261v1](https://arxiv.org/pdf/2602.05261v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CL |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: Length-Unbiased Sequence Policy Optimization

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ RLVR (Reinforcement Learning with Verifiable Rewards) ê¸°ë°˜ì˜ LLM ë° VLMsì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì— ëŒ€í•œ ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ë¥¼ ì§€ì í•˜ê³ , íŠ¹íˆ response length ì¦ê°€ê°€ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì— ë¯¸ì¹˜ëŠ” í¸í–¥ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤. ê¸°ì¡´ GSPO ì•Œê³ ë¦¬ì¦˜ì˜ length bias ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Length-Unbiased Sequence Policy Optimization (LUSPO) ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ìˆ˜í•™ì  ì¶”ë¡  ë° ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. LUSPOëŠ” response lengthì— ëŒ€í•œ í¸í–¥ì„ ì œê±°í•˜ì—¬ collapse ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ìƒˆë¡œìš´ ìµœì í™” ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ë³¸ ë…¼ë¬¸ì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ RLVR ì•Œê³ ë¦¬ì¦˜ì˜ response length bias ë¬¸ì œë¥¼ ëª…í™•í•˜ê²Œ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ LUSPO ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•œ ì ì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì´ response length ì¦ê°€ë¥¼ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì˜ í•µì‹¬ ìš”ì¸ìœ¼ë¡œ ê°„ì£¼í–ˆì§€ë§Œ, ê·¸ íŒ¨í„´ì˜ ë³€ë™ì„±ì„ ì œëŒ€ë¡œ ì„¤ëª…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LUSPOëŠ” GSPOì˜ length biasë¥¼ ìˆ˜ì •í•˜ì—¬ loss functionì„ unbiasedí•˜ê²Œ ë§Œë“¤ê³ , response length collapse ë¬¸ì œë¥¼ í•´ê²°í•¨ìœ¼ë¡œì¨ ìƒˆë¡œìš´ ìµœì í™” ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, response lengthì— ëŒ€í•œ í¸í–¥ì„ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ê²ƒì€ ê¸°ì¡´ ì—°êµ¬ì™€ ì°¨ë³„í™”ë˜ëŠ” ì ì…ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **Theoretical Analysis & Empirical Validation:** response length ë³€í™”ì˜ ì›ì¸ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ì‹¤í—˜ì„ í†µí•´ ê²€ì¦í•¨ìœ¼ë¡œì¨ ë¬¸ì œì˜ ê·¼ë³¸ì ì¸ ì›ì¸ì„ íŒŒì•…í•˜ê³  í•´ê²°ì±…ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.
2. **Length Bias í•´ê²°:** RLVR ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ì ì¸ ë¬¸ì œì ì¸ response length biasë¥¼ ëª…í™•í•˜ê²Œ ì§€ì í•˜ê³ , LUSPO ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
3. **State-of-the-Art Performance:** ìˆ˜í•™ì  ì¶”ë¡  ë° ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ìƒˆë¡œìš´ ìµœì í™” ì „ëµì˜ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **Specific RLVR Algorithms:** LUSPOëŠ” GSPOì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, ë‹¤ë¥¸ RLVR ì•Œê³ ë¦¬ì¦˜ì— ì ìš© ê°€ëŠ¥ì„±ì€ ëª…í™•í•˜ê²Œ ì œì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
2. **Limited Scope of Reasoning:** ì‹¤í—˜ì´ ìˆ˜í•™ì  ì¶”ë¡  ë° ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì—ë§Œ ì§‘ì¤‘ë˜ì–´ ìˆì–´, ë‹¤ë¥¸ ìœ í˜•ì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì— ëŒ€í•œ ì ìš© ê°€ëŠ¥ì„±ì€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **Detailed Algorithm Explanation:** LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ ë‚´ë¶€ ì‘ë™ ë°©ì‹ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì´ ë¶€ì¡±í•˜ë©°, êµ¬í˜„ ë° ì´í•´ë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì˜ ì—°êµ¬ëŠ” ì œê°€ ì§„í–‰í•˜ê³  ìˆëŠ” Multimodal LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë©ë‹ˆë‹¤. íŠ¹íˆ, Vision-Language Alignment ì—°êµ¬ì—ì„œ response lengthì˜ í¸í–¥ì´ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ëŠ” ë° í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ length bias í•´ê²° ê¸°ë²•ì„ ì ìš©í•˜ì—¬ MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, Vision Encoderì™€ Text LLM ê°„ì˜ ì •ë ¬ ì—°êµ¬ì—ì„œ response lengthë¥¼ ì œì–´í•˜ëŠ” ê²ƒì´ ëª¨ë¸ì˜ ì•ˆì •ì ì¸ í•™ìŠµì— ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜í™”:** LUSPO ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë¥¸ RLVR ì•Œê³ ë¦¬ì¦˜ì— ì ìš© ê°€ëŠ¥ì„±ì„ íƒìƒ‰í•˜ê³ , ì¼ë°˜í™”ëœ length-unbiased optimization ì „ëµì„ ê°œë°œí•©ë‹ˆë‹¤.
2. **Diverse Reasoning Tasks:** ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì¶”ë¡  (ì˜ˆ: commonsense reasoning, causal reasoning) ë²¤ì¹˜ë§ˆí¬ì—ì„œ LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , íŠ¹ì • ìœ í˜•ì˜ ì¶”ë¡ ì— ëŒ€í•œ ìµœì í™” ì „ëµì„ ê°œë°œí•©ë‹ˆë‹¤.
3. **Visualization of Response Length Dynamics:** LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ response lengthì˜ ë³€í™”ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , length biasì˜ ì›ì¸ì„ ë”ìš± ëª…í™•í•˜ê²Œ íŒŒì•…í•©ë‹ˆë‹¤.
4. **Integration with Vision-Language Alignment:** LUSPO ì•Œê³ ë¦¬ì¦˜ì„ Vision-Language Alignment ì—°êµ¬ì— ì ìš©í•˜ì—¬, multimodal LLMì˜ response lengthë¥¼ ì œì–´í•˜ê³ , ëª¨ë¸ì˜ ì•ˆì •ì ì¸ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤.
5. **Theoretical Analysis of Length Bias:** response length biasì˜ ë°œìƒ ì›ì¸ì„ ë”ìš± ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ë‹¤ì–‘í•œ RLVR ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë°œìƒí•˜ëŠ” length biasì˜ íŠ¹ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Reinforcement Learning with Verifiable Rewards (RLVR)
2. Length-Unbiased Sequence Policy Optimization (LUSPO)
3. Response Length Bias
4. Multimodal Reasoning
5. Vision-Language Alignment


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
