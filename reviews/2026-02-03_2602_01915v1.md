---
title: "VLM-Guided Experience Replay"
date: 2026-02-03
arxiv: "2602.01915v1"
category: "cs.LG"
model: "google/gemma-3-4b-it:free"
---

# VLM-Guided Experience Replay

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Elad Sharony, Tom Jurgenson, Orr Krupnik, Dotan Di Castro, Shie Mannor |
| **ë°œí‘œì¼** | 2026-02-02 |
| **arXiv** | [2602.01915v1](https://arxiv.org/pdf/2602.01915v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.LG |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: VLM-Guided Experience Replay

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL)ì—ì„œ ê²½í—˜ ì¬ìƒ ë²„í¼(replay buffer)ì˜ í™œìš© íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ Vision-Language Model(VLM)ì„ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ RL ì—°êµ¬ì—ì„œ ê²½í—˜ ì¬ìƒ ë²„í¼ëŠ” LLMê³¼ VLMì„ í†µí•©í•˜ëŠ” ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œì„ì—ë„ ë¶ˆêµ¬í•˜ê³ , VLMì„ í™œìš©í•˜ì—¬ ê²½í—˜ì„ ìš°ì„ ìˆœìœ„í™”í•˜ëŠ” ë°©ì‹ì€ ê±°ì˜ ì—°êµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ì‚¬ì „ í•™ìŠµëœ VLMì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ê²½í—˜ì—ì„œ ìœ ë§í•œ í•˜ìœ„ ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ í‰ê°€í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ì§€ì •í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆí•˜ëŠ” ë°©ë²•ì€ ê²Œì„ ë° ë¡œë´‡ ê³µí•™ ë¶„ì•¼ì—ì„œ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ì„±ê³µë¥ ì„ 11-52% í–¥ìƒì‹œí‚¤ê³  ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ 19-45% ê°œì„ í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ë³¸ ë…¼ë¬¸ì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ ê°•í™” í•™ìŠµì˜ ê²½í—˜ ì¬ìƒ ë²„í¼ì— VLMì„ í†µí•©í•˜ì—¬ ê²½í—˜ ìš°ì„ ìˆœìœ„í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ LLMê³¼ VLMì„ RLì˜ ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œì— í†µí•©í–ˆì§€ë§Œ, ê²½í—˜ ì¬ìƒ ë²„í¼ ìì²´ë¥¼ VLMì˜ ëŠ¥ë ¥ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ì‹ì€ ì²˜ìŒì…ë‹ˆë‹¤. íŠ¹íˆ, ì‚¬ì „ í•™ìŠµëœ VLMì„ fine-tuning ì—†ì´ í™œìš©í•˜ì—¬ ê²½í—˜ì„ í‰ê°€í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì€ íš¨ìœ¨ì ì¸ ë°©ë²•ë¡ ì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **íš¨ìœ¨ì ì¸ ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ:** VLMì„ í™œìš©í•˜ì—¬ ìœ ë§í•œ ê²½í—˜ì„ ì„ ë³„í•¨ìœ¼ë¡œì¨, ë¶ˆí•„ìš”í•œ ê²½í—˜ì„ ë²„í¼ì—ì„œ ì œê±°í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
2. **ì‚¬ì „ í•™ìŠµëœ VLM í™œìš©:**  VLMì„ fine-tuning ì—†ì´ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, í•™ìŠµ ë¹„ìš©ê³¼ ì‹œê°„ì„ ì ˆì•½í•˜ê³ , VLMì˜ ì¼ë°˜ì ì¸ ì§€ì‹ í™œìš© ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤.
3. **ë‹¤ì–‘í•œ ë„ë©”ì¸ ì ìš© ê°€ëŠ¥ì„±:** ê²Œì„ ë° ë¡œë´‡ ê³µí•™ ë¶„ì•¼ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ íš¨ê³¼ë¥¼ ì…ì¦í•˜ì—¬, ì ìš© ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **VLMì˜ ì„±ëŠ¥ ì˜ì¡´ì„±:** VLMì˜ ì„±ëŠ¥ì´ ê²½í—˜ ìš°ì„ ìˆœìœ„í™” ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, VLMì˜ í’ˆì§ˆì´ ì¤‘ìš”í•©ë‹ˆë‹¤. VLMì˜ í¸í–¥ì´ë‚˜ ì˜¤ë¥˜ëŠ” ê²°ê³¼ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **Frozen VLMì˜ í•œê³„:** ì‚¬ì „ í•™ìŠµëœ VLMì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, íŠ¹ì • RL í™˜ê²½ì— ìµœì í™”í•˜ê¸° ì–´ë µë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
3. **ì¶”ë¡  ëŠ¥ë ¥ì˜ ì œí•œ:** VLMì´ ë‹¨ìˆœíˆ ì‹œê°ì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²½í—˜ì„ í‰ê°€í•˜ê¸° ë•Œë¬¸ì—, ë³µì¡í•œ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ê¸°ëŠ” ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì˜ ì—°êµ¬ëŠ” ì œê°€ ì§„í–‰í•˜ê³  ìˆëŠ” Scientific MLLM ë° Reasoning MLLM ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, Vision-Language Alignment ì—°êµ¬ì™€ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì œê°€ í˜„ì¬ ì—°êµ¬í•˜ëŠ” ê²ƒì²˜ëŸ¼, VLMì„ í™œìš©í•˜ì—¬ ì‹œê°ì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ë³¸ ë…¼ë¬¸ì˜ ì ‘ê·¼ ë°©ì‹ì€ ìœ ìš©í•˜ê²Œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, Scientific Document Understanding ì—°êµ¬ì—ì„œ VLMì„ í™œìš©í•˜ì—¬ ê³¼í•™ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í—˜ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ë° ì ìš© ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **Scientific Domainì— íŠ¹í™”ëœ VLM ê°œë°œ:** ê³¼í•™ ë¶„ì•¼ì˜ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ Scientific Domainì— íŠ¹í™”ëœ VLMì„ ê°œë°œí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³¼í•™ ë¬¸ì„œì˜ ì´í•´ ë° ì‹¤í—˜ ê³„íš ìˆ˜ë¦½ì— í™œìš©í•©ë‹ˆë‹¤.
2. **ì¶”ë¡  ëŠ¥ë ¥ ê°•í™”:** VLMê³¼ LLMì„ ê²°í•©í•˜ì—¬, ì‹œê°ì  ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ëª¨ë¸ì„ ê°œë°œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, VLMì´ ì œì‹œí•œ ì‹œê°ì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë…¼ë¦¬ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•˜ë„ë¡ í•©ë‹ˆë‹¤.
3. **VLMì˜ í‰ê°€ ê¸°ì¤€ ë‹¤ì–‘í™”:** VLMì´ ê²½í—˜ì„ í‰ê°€í•  ë•Œ, ë‹¨ìˆœíˆ ì‹œê°ì  ìœ ì‚¬ì„±ë¿ë§Œ ì•„ë‹ˆë¼, ê³¼í•™ì  ì§€ì‹, ì‹¤í—˜ ì„¤ê³„, ê²°ê³¼ í•´ì„ ë“± ë‹¤ì–‘í•œ ê¸°ì¤€ì„ ê³ ë ¤í•˜ë„ë¡ í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Reinforcement Learning (RL)
2. Experience Replay Buffer
3. Vision-Language Model (VLM)
4. Vision-Language Alignment
5. Multimodal Reasoning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
