---
title: "VLM-Guided Experience Replay"
date: 2026-02-03
arxiv: "2602.01915v1"
category: "cs.LG"
model: "google/gemma-3-27b-it:free"
---

# VLM-Guided Experience Replay

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Elad Sharony, Tom Jurgenson, Orr Krupnik, Dotan Di Castro, Shie Mannor |
| **ë°œí‘œì¼** | 2026-02-02 |
| **arXiv** | [2602.01915v1](https://arxiv.org/pdf/2602.01915v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.LG |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

---

## ğŸ” AI ë¶„ì„

## VLM-Guided Experience Replay ë…¼ë¬¸ ë¶„ì„

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ ê°•í™” í•™ìŠµ(RL)ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ ë¦¬í”Œë ˆì´ ë²„í¼ì— VLMs(Vision-Language Models)ë¥¼ í™œìš©í•˜ì—¬ ê²½í—˜ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ VLMì„ íŒŒì¸íŠœë‹ ì—†ì´ ìë™ í‰ê°€ìë¡œ ì‚¬ìš©í•˜ì—¬ ìœ ë§í•œ ë¶€ë¶„ ê¶¤ì (sub-trajectories)ì„ ì‹ë³„í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬í•¨ìœ¼ë¡œì¨ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ê³ ìˆ˜ì¤€ ê³„íš ë° í•´ì„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ê²Œì„ ë° ë¡œë´‡ ì œì–´ ë“± ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì œì•ˆëœ ë°©ë²•ì€ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ ëŒ€ë¹„ í‰ê·  ì„±ê³µë¥ ì„ 11-52% í–¥ìƒì‹œí‚¤ê³  ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ 19-45% ê°œì„ í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” VLMì˜ ê°•ë ¥í•œ ì‹œê°-ì–¸ì–´ ì¶”ë¡  ëŠ¥ë ¥ì„ RLì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ê¸°ì¡´ ì—°êµ¬ë“¤ì´ LLMì´ë‚˜ VLMì„ RLì˜ ì •ì±…, ê°€ì¹˜ í•¨ìˆ˜, ë³´ìƒ í•¨ìˆ˜ ë“±ì— í†µí•©í•˜ëŠ” ë° ì§‘ì¤‘í•œ ë°˜ë©´, ë³¸ ë…¼ë¬¸ì€ **ë¦¬í”Œë ˆì´ ë²„í¼ë¼ëŠ” RLì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì— VLMì„ ì ìš©í•˜ì—¬ ê²½í—˜ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹**ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, **ì‚¬ì „ í•™ìŠµëœ VLMì„ íŒŒì¸íŠœë‹ ì—†ì´ í™œìš©**í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì‹¤ìš©ì„±ì„ ë†’ì˜€ë‹¤ëŠ” ì ì´ ì°¨ë³„ì ì…ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ:** VLMì„ í™œìš©í•œ ê²½í—˜ ìš°ì„ ìˆœìœ„ ê²°ì •ìœ¼ë¡œ RL ì—ì´ì „íŠ¸ì˜ í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê³  í•„ìš”í•œ ë°ì´í„° ì–‘ì„ ì¤„ì…ë‹ˆë‹¤.
2. **íŒŒì¸íŠœë‹ ë¶ˆí•„ìš”:** ì‚¬ì „ í•™ìŠµëœ VLMì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ì—¬ ì¶”ê°€ì ì¸ í•™ìŠµ ë¹„ìš©ê³¼ ë³µì¡ì„±ì„ ì¤„ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ í™˜ê²½ì— ë¹ ë¥´ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì„ ê°€ì§‘ë‹ˆë‹¤.
3. **ë‹¤ì–‘í•œ í™˜ê²½ ì ìš© ê°€ëŠ¥ì„±:** ê²Œì„ê³¼ ë¡œë´‡ ì œì–´ ë“± ì´ì‚° ë° ì—°ì†ì ì¸ ë„ë©”ì¸ì—ì„œ ëª¨ë‘ íš¨ê³¼ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì¼ë°˜ì ì¸ ì ìš© ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **VLM ì˜ì¡´ì„±:** ì„±ëŠ¥ì´ VLMì˜ ì„±ëŠ¥ì— í¬ê²Œ ì˜ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. VLMì´ íŠ¹ì • í™˜ê²½ì´ë‚˜ ì‘ì—…ì— ì í•©í•˜ì§€ ì•Šì€ ê²½ìš° ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ë¶€ë¶„ ê¶¤ì  ì‹ë³„ ê¸°ì¤€:** VLMì´ ìœ ë§í•œ ë¶€ë¶„ ê¶¤ì ì„ ì‹ë³„í•˜ëŠ” ê¸°ì¤€ì´ ëª…í™•í•˜ê²Œ ì„¤ëª…ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì–´ë–¤ ì‹œê°ì , ì–¸ì–´ì  íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.
3. **ë³µì¡í•œ í™˜ê²½ì—ì„œì˜ í™•ì¥ì„±:** ë³µì¡í•œ í™˜ê²½ì´ë‚˜ ì¥ê¸°ì ì¸ ê³„íšì´ í•„ìš”í•œ ì‘ì—…ì—ì„œ VLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì— í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì €ì˜ ì—°êµ¬ ë¶„ì•¼ì¸ Scientific MLLM, Reasoning MLLM, Vision-Language Alignmentì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, **VLMì˜ ì‹œê°-ì–¸ì–´ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ë°©ì‹**ì€ ì œê°€ í˜„ì¬ ì—°êµ¬í•˜ê³  ìˆëŠ” Scientific ë„ë©”ì¸ì—ì„œì˜ MLLM í™œìš©ê³¼ ìœ ì‚¬í•œ ë§¥ë½ì„ ê°€ì§‘ë‹ˆë‹¤. ë˜í•œ, **Vision encoderì™€ Text LLM ê°„ì˜ ì •ë ¬(alignment)ì„ í†µí•´ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬**ì— ì‹œì‚¬í•˜ëŠ” ë°”ê°€ í½ë‹ˆë‹¤. VLMì´ ê²½í—˜ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ëŠ” ê³¼ì •ì—ì„œ ì–´ë–¤ ì‹œê°ì , ì–¸ì–´ì  íŠ¹ì§•ì„ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•˜ëŠ”ì§€ ë¶„ì„í•˜ë©´, Vision encoderì™€ Text LLM ê°„ì˜ íš¨ê³¼ì ì¸ ì •ë ¬ ì „ëµì„ ê°œë°œí•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **Scientific ë„ë©”ì¸ ì ìš©:** ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì„ Scientific ë„ë©”ì¸(ì˜ˆ: í™”í•™ ì‹¤í—˜, ìƒë¬¼í•™ì  ì´ë¯¸ì§€ ë¶„ì„)ì— ì ìš©í•˜ì—¬ RL ì—ì´ì „íŠ¸ì˜ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **VLMì˜ íŒë‹¨ ê·¼ê±° ë¶„ì„:** VLMì´ ê²½í—˜ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ëŠ” ê·¼ê±°ë¥¼ ì‹œê°í™”í•˜ê³  ë¶„ì„í•˜ì—¬, ì–´ë–¤ ì‹œê°ì , ì–¸ì–´ì  íŠ¹ì§•ì´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ”ì§€ íŒŒì•…í•˜ëŠ” ì—°êµ¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Vision encoderì™€ Text LLM ê°„ì˜ alignmentë¥¼ ê°œì„ í•˜ëŠ” ë° í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **VLMì˜ ì¶”ë¡  ëŠ¥ë ¥ ê°•í™”:** VLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ í•™ìŠµì´ë‚˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì„ ì ìš©í•˜ê³ , RL ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **ë‹¤ì–‘í•œ VLM ë¹„êµ:** ë‹¤ì–‘í•œ VLM(ì˜ˆ: CLIP, Flamingo, LLaVA)ì„ ì‚¬ìš©í•˜ì—¬ ê²½í—˜ ìš°ì„ ìˆœìœ„ ê²°ì • ì„±ëŠ¥ì„ ë¹„êµí•˜ê³ , ê° VLMì˜ ì¥ë‹¨ì ì„ ë¶„ì„í•˜ëŠ” ì—°êµ¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Vision-Language Models (VLMs)
2. Reinforcement Learning (RL)
3. Experience Replay
4. Sample Efficiency
5. Multimodal Reasoning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
