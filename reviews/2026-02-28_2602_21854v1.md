---
title: "FewMMBench: A Benchmark for Multimodal Few-Shot Learning"
date: 2026-02-28
arxiv: "2602.21854v1"
category: "cs.CL"
model: "google/gemma-3-27b-it:free"
---

# FewMMBench: A Benchmark for Multimodal Few-Shot Learning

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Mustafa Dogan, Ilker Kesen, Iacer Calixto, Aykut Erdem, Erkut Erdem |
| **ë°œí‘œì¼** | 2026-02-25 |
| **arXiv** | [2602.21854v1](https://arxiv.org/pdf/2602.21854v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CL |

---

## ğŸ“ ì´ˆë¡ (Abstract)

As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench

---

## ğŸ” AI ë¶„ì„

## ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ Multimodal Large Language Model (MLLM)ì˜ Few-Shot Learning ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ FewMMBenchë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. FewMMBenchëŠ” ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ê³¼ì œë¥¼ í¬í•¨í•˜ë©°, In-Context Learning (ICL) ë° Chain-of-Thought (CoT) í”„ë¡¬í”„íŒ… ì „ëµ í•˜ì—ì„œ MLLMì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. 26ê°œì˜ ì˜¤í”ˆ ì›¨ì´íŠ¸ MLLMì„ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜í•œ ê²°ê³¼, instruction-tuned ëª¨ë¸ì€ zero-shot ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ì§€ë§Œ, ì¶”ê°€ì ì¸ ë°ëª¨ë‚˜ CoT ì¶”ë¡ ì„ í™œìš©í•´ë„ ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ì €í•˜ë˜ëŠ” ê²½í–¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” MLLMì˜ few-shot learning ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
FewMMBenchëŠ” ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ê°€ ì£¼ë¡œ zero-shot ì„±ëŠ¥ì— ì§‘ì¤‘í–ˆë˜ ê²ƒê³¼ ë‹¬ë¦¬, **MLLMì˜ few-shot learning ëŠ¥ë ¥, íŠ¹íˆ ICLê³¼ CoT í”„ë¡¬í”„íŒ… ì „ëµ í•˜ì—ì„œì˜ ì„±ëŠ¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ í‰ê°€**í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ë¼ëŠ” ì ì´ ê°€ì¥ í° ì°¨ë³„ì ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ê³¼ì œë¥¼ í¬í•¨í•˜ì—¬ MLLMì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í­ë„“ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬:** ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ê³¼ì œë¥¼ í¬í•¨í•˜ì—¬ MLLMì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **Few-Shot Learning ì§‘ì¤‘:** ICL ë° CoT í”„ë¡¬í”„íŒ… ì „ëµì„ í™œìš©í•˜ì—¬ MLLMì˜ few-shot learning ëŠ¥ë ¥ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
3. **ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ í‰ê°€:** 26ê°œì˜ ì˜¤í”ˆ ì›¨ì´íŠ¸ MLLMì„ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜í•˜ì—¬ ì—°êµ¬ ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ë†’ì´ê³ , ì»¤ë®¤ë‹ˆí‹°ì— ê¸°ì—¬í•©ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **í•œêµ­ì–´ ë°ì´í„° ë¶€ì¡±:** ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì´ ì˜ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ í•œêµ­ì–´ MLLMì˜ ì„±ëŠ¥ í‰ê°€ì—ëŠ” ì§ì ‘ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
2. **Task ë‹¤ì–‘ì„± ì œí•œ:** ë²¤ì¹˜ë§ˆí¬ì— í¬í•¨ëœ ê³¼ì œê°€ íŠ¹ì • ìœ í˜•ì— í¸í–¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ë¬¸ì„œ/ì°¨íŠ¸/OCR/í…Œì´ë¸” ì´í•´ì™€ ê°™ì€ íŠ¹ì • ë¶„ì•¼ì— ëŒ€í•œ í‰ê°€ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì˜ì¡´ì„±:** ICL ë° CoT í”„ë¡¬í”„íŒ… ì „ëµì˜ ì„±ëŠ¥ì€ í”„ë¡¬í”„íŠ¸ì˜ í’ˆì§ˆì— í¬ê²Œ ì˜ì¡´í•˜ë©°, ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì œê°€ ì§„í–‰í•˜ê³  ìˆëŠ” MLLM ì—°êµ¬, íŠ¹íˆ Vision encoderì™€ Text LLM ê°„ì˜ ì •ë ¬(alignment) ì—°êµ¬ì™€ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. Few-shot learning ëŠ¥ë ¥ì€ ëª¨ë¸ì˜ alignment ì •ë„ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œê°€ ë  ìˆ˜ ìˆìœ¼ë©°, FewMMBenchë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ alignment ì •ë„ë¥¼ ë¹„êµ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, MLLM mergingì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ì‹œë„í•˜ëŠ” ì—°êµ¬ì— ìˆì–´, few-shot learning ëŠ¥ë ¥ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ í•œêµ­ì–´ MLLM ê°œë°œ ë° í‰ê°€ì— ìˆì–´, FewMMBenchì˜ ë°ì´í„°ì…‹ êµ¬ì„± ë°©ì‹ì„ ì°¸ê³ í•˜ì—¬ í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ëŠ” ì•„ì´ë””ì–´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **í•œêµ­ì–´ Few-Shot MLLM ë²¤ì¹˜ë§ˆí¬ êµ¬ì¶•:** FewMMBenchì˜ ë°ì´í„°ì…‹ êµ¬ì„± ë°©ì‹ì„ ì°¸ê³ í•˜ì—¬ í•œêµ­ì–´ Few-Shot MLLM ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í•˜ê³ , í•œêµ­ì–´ MLLMì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
2. **DocVQA/ChartQA/Table VQA Task ì¶”ê°€:** FewMMBenchì— ë¬¸ì„œ/ì°¨íŠ¸/í…Œì´ë¸” ì´í•´ì™€ ê´€ë ¨ëœ DocVQA, ChartQA, Table VQA ê³¼ì œë¥¼ ì¶”ê°€í•˜ì—¬ MLLMì˜ íŠ¹ì • ë¶„ì•¼ ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
3. **MLLM Mergingê³¼ Few-Shot Learning ì„±ëŠ¥ ë³€í™” ë¶„ì„:** MLLM mergingì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ì‹œë„í•˜ê³ , few-shot learning ëŠ¥ë ¥ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ FewMMBenchë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.
4. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ìë™í™” ì—°êµ¬:** Few-shot learning ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ìë™í™” ì—°êµ¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Multimodal Large Language Model (MLLM)
2. Few-Shot Learning
3. In-Context Learning (ICL)
4. Chain-of-Thought (CoT)
5. Benchmark (FewMMBench)


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
