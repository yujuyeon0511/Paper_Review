---
title: "Beyond Transcripts: A Renewed Perspective on Audio Chaptering"
date: 2026-02-10
arxiv: "2602.08979v1"
category: "cs.SD"
model: "google/gemma-3-27b-it:free"
---

# Beyond Transcripts: A Renewed Perspective on Audio Chaptering

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Fabian Retkowski, Maike ZÃ¼fle, Thai Binh Nguyen, Jan Niehues, Alexander Waibel |
| **ë°œí‘œì¼** | 2026-02-09 |
| **arXiv** | [2602.08979v1](https://arxiv.org/pdf/2602.08979v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.SD |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.

---

## ğŸ” AI ë¶„ì„

## ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ ì¥ì‹œê°„ ì˜¤ë””ì˜¤ë¥¼ ì˜ë¯¸ ìˆëŠ” ì„¹ì…˜ìœ¼ë¡œ ìë™ ë¶„í• í•˜ëŠ” ì˜¤ë””ì˜¤ ì±•í„°ë§(Audio Chaptering) ì—°êµ¬ë¡œ, ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—°êµ¬ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì˜¤ë””ì˜¤ ì •ë³´ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ê¸°ë°˜ ëª¨ë¸, ìŒí–¥ íŠ¹ì§•, AudioSegë¼ëŠ” ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ ì „ìš© ì•„í‚¤í…ì²˜, ê·¸ë¦¬ê³  ë©€í‹°ëª¨ë‹¬ LLMì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì„ ì‹¤ì¦ì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, í…ìŠ¤íŠ¸ ì˜ì¡´ì ì¸ í‰ê°€ ë°©ì‹ê³¼ í…ìŠ¤íŠ¸ ë…ë¦½ì ì¸ í‰ê°€ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ë©°, ì˜¤ë””ì˜¤ ì±•í„°ë§ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
* **ì˜¤ë””ì˜¤ ì „ìš© ì•„í‚¤í…ì²˜ (AudioSeg) ì œì•ˆ:** í…ìŠ¤íŠ¸ ì •ë³´ ì—†ì´ ì˜¤ë””ì˜¤ íŠ¹ì§•ë§Œìœ¼ë¡œ ì±•í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•˜ì—¬ í…ìŠ¤íŠ¸ ì˜ì¡´ì ì¸ ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ë¥¼ ê·¹ë³µí–ˆìŠµë‹ˆë‹¤.
* **ë©€í‹°ëª¨ë‹¬ LLMì˜ ì˜¤ë””ì˜¤ ì±•í„°ë§ ì ìš© ì‹œë„:** ë©€í‹°ëª¨ë‹¬ LLMì„ ì˜¤ë””ì˜¤ ì±•í„°ë§ì— ì ìš©í•˜ê³ , ê·¸ í•œê³„ì (context length, instruction following)ì„ ëª…í™•íˆ ë°í˜”ìŠµë‹ˆë‹¤.
* **í‰ê°€ í”„ë¡œí† ì½œì˜ ë‹¤ì–‘í™”:** í…ìŠ¤íŠ¸ ì˜ì¡´ì ì¸ í‰ê°€ ë°©ì‹ê³¼ í…ìŠ¤íŠ¸ ë…ë¦½ì ì¸ í‰ê°€ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•˜ê³ , ì˜¤ë””ì˜¤ ì±•í„°ë§ ì—°êµ¬ì— ì í•©í•œ í‰ê°€ í”„ë¡œí† ì½œì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ì‹¤í—˜ì  ë¶„ì„ì˜ ì²´ê³„ì„±:** ë‹¤ì–‘í•œ ëª¨ë¸(í…ìŠ¤íŠ¸ ê¸°ë°˜, ì˜¤ë””ì˜¤ ì „ìš©, ë©€í‹°ëª¨ë‹¬ LLM)ì„ ë¹„êµ ë¶„ì„í•˜ê³ , ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸(transcript quality, acoustic features, duration, speaker composition)ì„ ì‹¤ì¦ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì—°êµ¬ ê²°ê³¼ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.
2. **ì˜¤ë””ì˜¤ ì •ë³´ í™œìš©ì˜ ì¤‘ìš”ì„± ê°•ì¡°:** í…ìŠ¤íŠ¸ ì •ë³´ ì—†ì´ ì˜¤ë””ì˜¤ ì •ë³´ë§Œìœ¼ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì˜¤ë””ì˜¤ ì •ë³´ í™œìš©ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í–ˆìŠµë‹ˆë‹¤.
3. **í‰ê°€ ë°©ë²•ë¡ ì˜ ê°œì„ :** í…ìŠ¤íŠ¸ ì˜ì¡´ì ì¸ í‰ê°€ ë°©ì‹ì˜ í•œê³„ë¥¼ ì§€ì í•˜ê³ , í…ìŠ¤íŠ¸ ë…ë¦½ì ì¸ í‰ê°€ ë°©ì‹ì˜ í•„ìš”ì„±ì„ ì œì‹œí•˜ì—¬ ì˜¤ë””ì˜¤ ì±•í„°ë§ ì—°êµ¬ì˜ í‰ê°€ ë°©ë²•ë¡  ë°œì „ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
* **ë©€í‹°ëª¨ë‹¬ LLM ì„±ëŠ¥ì˜ ì œí•œ:** ë©€í‹°ëª¨ë‹¬ LLMì´ context lengthì™€ instruction followingì˜ í•œê³„ë¡œ ì¸í•´ ì§§ì€ ì˜¤ë””ì˜¤ì—ì„œë§Œ ìœ ë§í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ì ì€ ì•„ì‰¬ìš´ ë¶€ë¶„ì…ë‹ˆë‹¤.
* **AudioSeg ì•„í‚¤í…ì²˜ì˜ ë³µì¡ì„±:** AudioSeg ì•„í‚¤í…ì²˜ì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ì‹ê³¼ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ ì„¤ëª…ì´ ë¶€ì¡±í•˜ì—¬ ì¬í˜„ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **Scientific ë„ë©”ì¸ ì ìš© ê°€ëŠ¥ì„±:** YTSeg ë°ì´í„°ì…‹ì€ ì¼ë°˜ì ì¸ ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ìœ¼ë¡œ, Scientific ë„ë©”ì¸(ê°•ì˜, ê³¼í•™ ë°œí‘œ ë“±)ì—ì„œì˜ ì„±ëŠ¥ì€ ê²€ì¦ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì €ì˜ ì—°êµ¬ ë¶„ì•¼ì¸ Scientific MLLM, Vision-Language Alignment, MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ ì—°êµ¬ì™€ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤. íŠ¹íˆ, ë©€í‹°ëª¨ë‹¬ LLMì„ í™œìš©í•œ ì˜¤ë””ì˜¤ ì±•í„°ë§ ì—°êµ¬ëŠ” Scientific ë„ë©”ì¸ì—ì„œ ê°•ì˜ë‚˜ ë°œí‘œ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ìš”ì•½í•˜ê³  ì±•í„°ë§í•˜ëŠ” ë° í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, AudioSeg ì•„í‚¤í…ì²˜ëŠ” Vision Encoderì™€ ìœ ì‚¬í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ë©°, ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œê³¼ LLM ê°„ì˜ ì •ë ¬(alignment) ì—°êµ¬ì— ì ìš©ë  ìˆ˜ ìˆëŠ” ì•„ì´ë””ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
* **Scientific ë„ë©”ì¸ ì˜¤ë””ì˜¤ ì±•í„°ë§ ì—°êµ¬:** Scientific ë„ë©”ì¸(ê°•ì˜, ê³¼í•™ ë°œí‘œ ë“±)ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ LLMê³¼ AudioSeg ì•„í‚¤í…ì²˜ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•˜ê³ , Scientific ë„ë©”ì¸ì— íŠ¹í™”ëœ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ê°œë°œí•˜ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **Vision-Language Alignment ê¸°ë°˜ ì˜¤ë””ì˜¤ ì±•í„°ë§:** ê°•ì˜ ìŠ¬ë¼ì´ë“œì™€ ì˜¤ë””ì˜¤ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ì˜¤ë””ì˜¤ ì±•í„°ë§ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **AudioSeg ì•„í‚¤í…ì²˜ì˜ Vision Encoder ì ìš©:** AudioSeg ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ Vision Encoderì— ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , Vision-Language Alignment ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Audio Chaptering
2. Multimodal LLM
3. Acoustic Features
4. Vision-Language Alignment (ê°„ì ‘ì )
5. Audio Representation Learning (AudioSeg)


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
