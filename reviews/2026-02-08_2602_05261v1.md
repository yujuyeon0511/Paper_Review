---
title: "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response"
date: 2026-02-08
arxiv: "2602.05261v1"
category: "cs.CL"
model: "google/gemma-3-4b-it:free"
---

# Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Fanfan Liu, Youyang Yin, Peng Shi, Siqi Yang, Zhixiong Zeng... |
| **ë°œí‘œì¼** | 2026-02-05 |
| **arXiv** | [2602.05261v1](https://arxiv.org/pdf/2602.05261v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CL |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ Reinforcement Learning with Verifiable Rewards (RLVR)ë¥¼ í™œìš©í•œ LLM ë° VLMsì˜ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ ì—°êµ¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. íŠ¹íˆ, RLVR í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì‘ë‹µ ê¸¸ì´ ë³€í™”ì˜ ë³€ë™ì„±ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ê·¸ ì›ì¸ì„ ì´ë¡ ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ì¡´ Group Sequence Policy Optimization (GSPO)ì˜ ê¸¸ì´ í¸í–¥ì„ í•´ê²°í•˜ëŠ” Length-Unbiased Sequence Policy Optimization (LUSPO) ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ë©°, ìˆ˜í•™ì  ì¶”ë¡  ë° ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì—ì„œ ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. í•µì‹¬ì€ ì‘ë‹µ ê¸¸ì´ ë³€í™”ì— ëŒ€í•œ í¸í–¥ì„ ì œê±°í•˜ì—¬ ì‘ë‹µ ê¸¸ì´ ë¶•ê´´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ìˆìŠµë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ë³¸ ë…¼ë¬¸ì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ RLVR í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì‘ë‹µ ê¸¸ì´ í¸í–¥ ë¬¸ì œë¥¼ ëª…í™•í•˜ê²Œ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ LUSPO ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•œ ì ì…ë‹ˆë‹¤. ê¸°ì¡´ RLVR ì•Œê³ ë¦¬ì¦˜(GSPO)ì˜ ê¸¸ì´ í¸í–¥ì„ ë‹¨ìˆœíˆ ê°œì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì´ë¡ ì ì¸ ë¶„ì„ì„ í†µí•´ í¸í–¥ì˜ ê·¼ë³¸ì ì¸ ì›ì¸ì„ íŒŒì•…í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ê¸¸ì´ ë³€í™”ì— ëŒ€í•œ í¸í–¥ì´ ì—†ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‘ë‹µ ê¸¸ì´ ë¶•ê´´ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ê³ , RLVR ê¸°ë°˜ ëª¨ë¸ì˜ ì•ˆì •ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ì´ë¡ ì  ê¸°ë°˜:** ì‘ë‹µ ê¸¸ì´ ë³€í™”ì˜ ì›ì¸ì„ ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë¬¸ì œ í•´ê²°ì˜ ê·¼ë³¸ì ì¸ ì´ìœ ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ê²½í—˜ì ì¸ ê°œì„ ì„ ë„˜ì–´, ë¬¸ì œì˜ í•µì‹¬ì„ íŒŒì•…í•˜ê³  í•´ê²°ì±…ì„ ì œì‹œí•˜ëŠ” ì ì´ ê°•ì ì…ë‹ˆë‹¤.
2. **LUSPO ì•Œê³ ë¦¬ì¦˜:** ê¸°ì¡´ GSPOì˜ ê¸¸ì´ í¸í–¥ì„ í•´ê²°í•˜ê³  ì‘ë‹µ ê¸¸ì´ ë¶•ê´´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” LUSPO ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì—¬, RLVR ê¸°ë°˜ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.
3. **ì‹¤í—˜ì  ê²€ì¦:** ìˆ˜í•™ì  ì¶”ë¡  ë° ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ë¶„ì•¼ì—ì„œ LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ ìš°ìˆ˜ì„±ì„ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í•˜ì—¬, ì œì•ˆëœ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **íŠ¹ì • RLVR ì•Œê³ ë¦¬ì¦˜ì— ì§‘ì¤‘:** LUSPOëŠ” GSPOë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë¯€ë¡œ, ë‹¤ë¥¸ RLVR ì•Œê³ ë¦¬ì¦˜ì— ì ìš© ê°€ëŠ¥ì„±ì€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ì‘ë‹µ ê¸¸ì´ ì™¸ ë‹¤ë¥¸ ìš”ì¸ ê°„ê³¼ ê°€ëŠ¥ì„±:** ì‘ë‹µ ê¸¸ì´ ë³€í™”ì˜ ì›ì¸ì„ ë¶„ì„í–ˆì§€ë§Œ, ë‹¤ë¥¸ ìš”ì¸(ì˜ˆ: ëª¨ë¸ êµ¬ì¡°, í•™ìŠµ ë°ì´í„°)ì´ ì‘ë‹µ ê¸¸ì´ ë³€í™”ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì¶©ë¶„íˆ ê³ ë ¤ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì˜ ë²”ìœ„ ì œí•œ:** ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ë¶„ì•¼ì—ì„œ ì‹¤í—˜ì„ ì§„í–‰í–ˆì§€ë§Œ, ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ë° ì‘ì—…ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì€ ì•„ì§ ê²€ì¦ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì˜ ì—°êµ¬ëŠ” Multimodal Large Language Models (MLLM) ë¶„ì•¼ì—ì„œ íŠ¹íˆ Vision-Language Alignment ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë©ë‹ˆë‹¤. íŠ¹íˆ, Vision Encoderì™€ Text LLM ê°„ì˜ ì •ë ¬ ì—°êµ¬ì—ì„œ ì‘ë‹µ ê¸¸ì´ ë³€í™”ê°€ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ ê°œë°œì€ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì—°êµ¬ì™€ ì‹œë„ˆì§€ë¥¼ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ ì´ë¡ ì  ë¶„ì„ì€ Vision-Language Alignment ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ê¸¸ì´ í¸í–¥ ë¬¸ì œë¥¼ ì´í•´í•˜ê³  í•´ê²°í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **LUSPO ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜í™”:** LUSPO ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë¥¸ RLVR ì•Œê³ ë¦¬ì¦˜ì— ì ìš©í•˜ì—¬, ë‹¤ì–‘í•œ RLVR ê¸°ë°˜ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥ì„±ì„ íƒìƒ‰í•©ë‹ˆë‹¤.
2. **ì‘ë‹µ ê¸¸ì´ ì™¸ ë‹¤ë¥¸ ìš”ì¸ê³¼ì˜ ì—°ê´€ì„± ë¶„ì„:** ì‘ë‹µ ê¸¸ì´ ë³€í™”ì˜ ì›ì¸ì„ ë¶„ì„í•  ë•Œ, ëª¨ë¸ êµ¬ì¡°, í•™ìŠµ ë°ì´í„°, reward function ë“± ë‹¤ë¥¸ ìš”ì¸ê³¼ì˜ ì—°ê´€ì„±ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬, ë³´ë‹¤ í¬ê´„ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3. **ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì—ì„œì˜ LUSPO ì ìš©:** ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹(ì˜ˆ: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸, ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸)ì—ì„œ LUSPO ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬, ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
4. **Length-Unbiased Policy Optimizationì˜ í™•ì¥:** LUSPOì˜ Length-Unbiased ê°œë…ì„ ë‹¤ë¥¸ ìµœì í™” ë¬¸ì œì— ì ìš©í•˜ì—¬, ìƒˆë¡œìš´ ìµœì í™” ì „ëµì„ ê°œë°œí•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Reinforcement Learning with Verifiable Rewards (RLVR)
2. Length-Unbiased Sequence Policy Optimization (LUSPO)
3. Response Length Collapse
4. Vision-Language Alignment
5. Multimodal Reasoning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
