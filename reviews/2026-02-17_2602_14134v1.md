---
title: "DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors"
date: 2026-02-17
arxiv: "2602.14134v1"
category: "cs.CV"
model: "google/gemma-3-4b-it:free"
---

# DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Yi Li, Hongze Shen, Lexiang Tang, Xin Li, Xinpeng Ding... |
| **ë°œí‘œì¼** | 2026-02-15 |
| **arXiv** | [2602.14134v1](https://arxiv.org/pdf/2602.14134v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CV |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: DenseMLLM

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
DenseMLLM ë…¼ë¬¸ì€ ê¸°ì¡´ MLLMë“¤ì´ ê³ ìˆ˜ì¤€ ì‹œê°ì  ì´í•´ì—ëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, ì„¸ë°€í•œ ë°€ë„ ì˜ˆì¸¡ (semantic segmentation, depth estimation ë“±) ì‘ì—…ì—ëŠ” ì¶”ê°€ì ì¸ task-specific ë””ì½”ë”ê°€ í•„ìš”í•˜ë‹¤ëŠ” ë¬¸ì œì ì„ ì§€ì í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ í‘œì¤€ MLLM ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì—¬ ë³„ë„ì˜ ë””ì½”ë” ì—†ì´ ë°€ë„ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” DenseMLLM ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. í•µì‹¬ì€ vision token supervision ì „ëµì„ í†µí•´ ë‹¤ì¤‘ ë ˆì´ë¸” ë° ì‘ì—…ì— ëŒ€í•œ íš¨ê³¼ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, DenseMLLMì€ ë‹¤ì–‘í•œ ë°€ë„ ì˜ˆì¸¡ ë° ë¹„ì „-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, MLLMì˜ ì¼ë°˜ì ì¸ ë””ìì¸ì„ ìœ ì§€í•˜ë©´ì„œë„ ë°€ë„ ì˜ˆì¸¡ì„ ì§€ì›í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦í•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ í‘œì¤€ MLLM ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì—¬ ë°€ë„ ì˜ˆì¸¡ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ MLLMì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ task-specific ë””ì½”ë”ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì— ì§‘ì¤‘í–ˆì§€ë§Œ, DenseMLLMì€ MLLMì˜ ì¼ë°˜ì ì¸ ë””ìì¸ì„ ìœ ì§€í•˜ë©´ì„œë„ ë°€ë„ ì˜ˆì¸¡ì„ ì§€ì›í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, vision token supervision ì „ëµì„ í†µí•´ ë‹¤ì¤‘ ë ˆì´ë¸” ë° ì‘ì—…ì— ëŒ€í•œ í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì ì´ ì£¼ëª©í•  ë§Œí•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ê°„ê²°í•œ ì•„í‚¤í…ì²˜:** DenseMLLMì€ ë³µì¡í•œ task-specific ë””ì½”ë”ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šê³  í‘œì¤€ MLLM ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¤„ì´ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
2. **ë¹„ì „-í…ìŠ¤íŠ¸ ì •ë ¬ íš¨ìœ¨ì„±:** vision token supervision ì „ëµì„ í†µí•´ ë¹„ì „ ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì •ë ¬í•˜ê³  í•™ìŠµí•˜ì—¬ ë°€ë„ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
3. **ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ê²½ìŸë ¥:** ë‹¤ì–‘í•œ ë°€ë„ ì˜ˆì¸¡ ë° ë¹„ì „-ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì–´ ëª¨ë¸ì˜ ì‹¤ìš©ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **íŠ¹ì • ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ì¡´ì„±:** DenseMLLMì˜ ì„±ëŠ¥ì€ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë©°, íŠ¹ì • ë°ì´í„°ì…‹ì— ê³¼ì í•©ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
2. **ë¹„ì „-í…ìŠ¤íŠ¸ ì •ë ¬ ì „ëµì˜ í•œê³„:** vision token supervision ì „ëµì´ ëª¨ë“  ë°€ë„ ì˜ˆì¸¡ ì‘ì—…ì— íš¨ê³¼ì ì¸ ê²ƒì€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë©°, íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì— ëŒ€í•œ ì œí•œì ì¸ ì—°êµ¬:** ë…¼ë¬¸ì€ ë°€ë„ ì˜ˆì¸¡ ì‘ì—…ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, ì‹¤ì œ ì‘ìš© ë¶„ì•¼ (ì˜ˆ: ììœ¨ ì£¼í–‰, ë¡œë´‡ ê³µí•™)ì—ì„œì˜ í™œìš© ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì—°êµ¬ëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ì´ ë…¼ë¬¸ì˜ DenseMLLMì€ í˜„ì¬ ì§„í–‰í•˜ê³  ìˆëŠ” Vision-Text Alignment ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë©ë‹ˆë‹¤. íŠ¹íˆ, vision token supervision ì „ëµì€ ë‹¤ì–‘í•œ multimodal ë°ì´í„°ì˜ ì •ë ¬ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ë°©ë²•ë¡ ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, í•œêµ­ì–´ Multimodal LLM ê°œë°œì„ ìœ„í•œ ì—°êµ¬ì—ë„ ì ìš©í•˜ì—¬, í•œêµ­ì–´ ë¬¸ì„œ/ì°¨íŠ¸/OCR/í…Œì´ë¸” ì´í•´ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  Vision encoderì™€ Text LLM ê°„ì˜ ì •ë ¬ ì—°êµ¬ë¥¼ í†µí•´ DenseMLLMì˜ íš¨ê³¼ë¥¼ ë”ìš± ê·¹ëŒ€í™”í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **ë‹¤ì–‘í•œ ë°€ë„ ì˜ˆì¸¡ ì‘ì—…ì— ëŒ€í•œ ì ìš©:** DenseMLLMì„ ë‹¤ì–‘í•œ ë°€ë„ ì˜ˆì¸¡ ì‘ì—… (ì˜ˆ: 3D ë°€ë„ ì˜ˆì¸¡, ì‹œê°ì  ì§ˆê° ì˜ˆì¸¡)ì— ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê°œì„ í•©ë‹ˆë‹¤.
2. **ìƒˆë¡œìš´ ë¹„ì „-í…ìŠ¤íŠ¸ ì •ë ¬ ì „ëµ ê°œë°œ:** vision token supervision ì „ëµì„ ê°œì„ í•˜ê±°ë‚˜, ë‹¤ë¥¸ ì •ë ¬ ì „ëµ (ì˜ˆ: contrastive learning)ì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚µë‹ˆë‹¤.
3. **í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ Fine-tuning:** í•œêµ­ì–´ ë¬¸ì„œ/ì°¨íŠ¸/OCR/í…Œì´ë¸” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ DenseMLLMì„ Fine-tuningí•˜ì—¬ í•œêµ­ì–´ Multimodal LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
4. **ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì— ëŒ€í•œ ì‹¤í—˜:** ììœ¨ ì£¼í–‰, ë¡œë´‡ ê³µí•™ ë“± ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œ DenseMLLMì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ë¬¸ì œì ì„ ê°œì„ í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. MLLM (Multimodal Large Language Models)
2. Dense Prediction
3. Vision Token Supervision
4. Vision-Text Alignment
5. Model Merging


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
