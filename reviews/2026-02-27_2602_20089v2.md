---
title: "StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues"
date: 2026-02-27
arxiv: "2602.20089v2"
category: "cs.CV"
model: "google/gemma-3-27b-it:free"
---

# StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Zanxi Ruan, Qiuyu Kong, Songqun Gao, Yiming Wang, Marco Cristani |
| **ë°œí‘œì¼** | 2026-02-23 |
| **arXiv** | [2602.20089v2](https://arxiv.org/pdf/2602.20089v2) |
| **ì¹´í…Œê³ ë¦¬** | cs.CV |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StruXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them "structure-centric". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StruXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StruXLIP.

---

## ğŸ” AI ë¶„ì„

## StruXLIP ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ ì‹œê°ì  ì´í•´ì˜ ê¸°ë³¸ ìš”ì†Œì¸ edge ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ Vision-Language ëª¨ë¸ì˜ ì •ë ¬(alignment) ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” StruXLIPì´ë¼ëŠ” ìƒˆë¡œìš´ íŒŒì¸íŠœë‹ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ edge mapì„ ì¶”ì¶œí•˜ê³ , ì´ì— ëŒ€ì‘í•˜ëŠ” ìº¡ì…˜ì„ êµ¬ì¡°ì  íŠ¹ì§•ì„ ê°•ì¡°í•˜ë„ë¡ í•„í„°ë§í•˜ì—¬ êµ¬ì¡° ì¤‘ì‹¬ì ì¸ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ cross-modal retrieval ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ë” ë‚˜ì•„ê°€ ëª¨ë¸ì´ ë” robustí•˜ê³  ì•ˆì •ì ì¸ í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
* **êµ¬ì¡° ì¤‘ì‹¬ì  ì •ë ¬(Structure-centric Alignment):** ê¸°ì¡´ CLIP ëª¨ë¸ì´ ì‹œê°-í…ìŠ¤íŠ¸ ì„ë² ë”© ê°„ì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ìµœëŒ€í™”í•˜ëŠ” ë° ì§‘ì¤‘í•œ ë°˜ë©´, ë³¸ ë…¼ë¬¸ì€ multimodal êµ¬ì¡°ì  í‘œí˜„ ê°„ì˜ ìƒí˜¸ ì •ë³´ëŸ‰ê¹Œì§€ ìµœëŒ€í™”í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.
* **Edge Map í™œìš©:** ì´ë¯¸ì§€ì˜ edge mapì„ ì‹œê°ì  êµ¬ì¡°ì˜ proxyë¡œ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì •ë ¬í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” ì´ë¯¸ì§€ì˜ ì˜ë¯¸ë¡ ì  ì •ë³´ë¿ë§Œ ì•„ë‹ˆë¼ êµ¬ì¡°ì  ì •ë³´ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ì •ë ¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
* **ì„¸ ê°€ì§€ êµ¬ì¡° ì¤‘ì‹¬ì  ì†ì‹¤ í•¨ìˆ˜:** edge mapê³¼ êµ¬ì¡°ì  í…ìŠ¤íŠ¸ ì •ë ¬, local edge regionê³¼ í…ìŠ¤íŠ¸ chunk ë§¤ì¹­, edge mapê³¼ ì»¬ëŸ¬ ì´ë¯¸ì§€ ì—°ê²°ì„ ìœ„í•œ ì„¸ ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ë„í•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ì„±ëŠ¥ í–¥ìƒ:** Cross-modal retrieval taskì—ì„œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ, ì¼ë°˜ì ì¸ ë„ë©”ì¸ë¿ë§Œ ì•„ë‹ˆë¼ íŠ¹ì • ë„ë©”ì¸ì—ì„œë„ íš¨ê³¼ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.
2. **ì¼ë°˜í™” ê°€ëŠ¥ì„±:** StruXLIPì€ plug-and-play ë°©ì‹ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì— ì‰½ê²Œ í†µí•©ë  ìˆ˜ ìˆì–´, ë‹¤ì–‘í•œ Vision-Language ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
3. **ì´ë¡ ì  ê·¼ê±°:** ìƒí˜¸ ì •ë³´ëŸ‰ ìµœëŒ€í™”ë¼ëŠ” ì´ë¡ ì  ë°°ê²½ì„ ë°”íƒ•ìœ¼ë¡œ, ëª¨ë¸ì´ ë” robustí•˜ê³  ì•ˆì •ì ì¸ í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì œì‹œí•©ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **Edge Map ì¶”ì¶œ ë°©ë²• ì˜ì¡´ì„±:** Canny edge detectorë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, ë‹¤ë¥¸ edge detection ë°©ë²•ì˜ ì„±ëŠ¥ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ edge detection ë°©ë²•ì˜ ë¹„êµ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.
2. **êµ¬ì¡° ì¤‘ì‹¬ì  ìº¡ì…˜ í•„í„°ë§ì˜ íš¨ê³¼:** êµ¬ì¡°ì  íŠ¹ì§•ì„ ê°•ì¡°í•˜ë„ë¡ ìº¡ì…˜ì„ í•„í„°ë§í•˜ëŠ” ê³¼ì •ì´ ì‹¤ì œë¡œ ì„±ëŠ¥ í–¥ìƒì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ ëª…í™•í•˜ê²Œ ë¶„ì„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
3. **í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€ ë¶€ì¬:** ë³¸ ë…¼ë¬¸ì€ ì£¼ë¡œ ì˜ì–´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë˜ì—ˆìœ¼ë©°, í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ê°€ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì œ ì—°êµ¬ ë¶„ì•¼ì¸ Vision-Text Alignment, íŠ¹íˆ í•œêµ­ì–´ Multimodal LLM ê°œë°œì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤. Edge ì •ë³´ë¥¼ í™œìš©í•œ êµ¬ì¡° ì¤‘ì‹¬ì  ì •ë ¬ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ë¯¸ë¬˜í•œ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë¬¸ì„œ/ì°¨íŠ¸/OCR/í…Œì´ë¸” ì´í•´ íŠ¹í™” MLLM ê°œë°œì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, í•œêµ­ì–´ ë¬¸ì„œëŠ” ì‹œê°ì  ìš”ì†Œì™€ í…ìŠ¤íŠ¸ ì •ë³´ê°€ ë³µì¡í•˜ê²Œ ì–½í˜€ ìˆëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ, edge ì •ë³´ë¥¼ í™œìš©í•œ ì •ë ¬ì€ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, MLLM merging ì—°êµ¬ì— ìˆì–´, StruXLIPì˜ êµ¬ì¡° ì¤‘ì‹¬ì  ì†ì‹¤ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ ê°„ì˜ ì •ë ¬ì„ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ í™œìš©í•œ StruXLIP ì„±ëŠ¥ í‰ê°€:** í•œêµ­ì–´ ë¬¸ì„œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹(ì˜ˆ: OCR ë°ì´í„°ì…‹, ì°¨íŠ¸ ì´ë¯¸ì§€ ë°ì´í„°ì…‹)ì„ êµ¬ì¶•í•˜ê³ , StruXLIPì„ íŒŒì¸íŠœë‹í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
2. **ë‹¤ì–‘í•œ Edge Detection ë°©ë²• ë¹„êµ ë¶„ì„:** Canny edge detector ì™¸ì— ë‹¤ë¥¸ edge detection ë°©ë²•(ì˜ˆ: Sobel, Laplacian)ì„ ì‚¬ìš©í•˜ì—¬ StruXLIPì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
3. **êµ¬ì¡° ì¤‘ì‹¬ì  ìº¡ì…˜ í•„í„°ë§ ë°©ë²• ê°œì„ :** ìº¡ì…˜ í•„í„°ë§ ê³¼ì •ì—ì„œ êµ¬ì¡°ì  íŠ¹ì§•ì„ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ê°•ì¡°í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. (ì˜ˆ: attention mechanism í™œìš©)
4. **StruXLIPê³¼ MLLM Merging ê²°í•©:** MLLM merging ê³¼ì •ì—ì„œ StruXLIPì˜ êµ¬ì¡° ì¤‘ì‹¬ì  ì†ì‹¤ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ ê°„ì˜ ì •ë ¬ì„ ê°•í™”í•˜ê³ , ìµœì¢… ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
5. **DocVQA/ChartQA taskì— StruXLIP ì ìš©:** ë¬¸ì„œ/ì°¨íŠ¸ ì´í•´ taskì— StruXLIPì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ ì‹œë„í•©ë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Vision-Language Alignment
2. Edge Detection
3. Cross-Modal Retrieval
4. Structure-centric Learning
5. Multimodal Representation Learning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
