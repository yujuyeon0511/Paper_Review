---
title: "StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues"
date: 2026-03-01
arxiv: "2602.20089v2"
category: "cs.CV"
model: "google/gemma-3-4b-it:free"
---

# StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Zanxi Ruan, Qiuyu Kong, Songqun Gao, Yiming Wang, Marco Cristani |
| **ë°œí‘œì¼** | 2026-02-23 |
| **arXiv** | [2602.20089v2](https://arxiv.org/pdf/2602.20089v2) |
| **ì¹´í…Œê³ ë¦¬** | cs.CV |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StruXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them "structure-centric". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StruXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StruXLIP.

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
StruXLIP ë…¼ë¬¸ì€ ì´ë¯¸ì§€ì˜ ì‹œê°ì  êµ¬ì¡°ë¥¼ ê°•ì¡°í•˜ì—¬ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ ì •ë ¬ì„ ê°œì„ í•˜ëŠ” ìƒˆë¡œìš´ Fine-tuning Alignment íŒŒë¼ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. Canny edge mapê³¼ ê°™ì€ ì‹œê°ì  êµ¬ì¡° ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìº¡ì…˜ì— êµ¬ì¡° ì¤‘ì‹¬ì ì¸ ì •ë³´ë¥¼ í•„í„°ë§í•˜ê³ , ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì‹œê°ì  êµ¬ì¡°ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìƒí˜¸ ì •ë³´(mutual information)ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤. ê¸°ì¡´ CLIP ëª¨ë¸ì˜ ìƒí˜¸ ì •ë³´ ìµœëŒ€í™” ë°©ì‹ì— ë”í•˜ì—¬, ë‹¤ì¤‘ ëª¨ë‹¬ êµ¬ì¡°ì  í‘œí˜„ ê°„ì˜ ìƒí˜¸ ì •ë³´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë³´ì¡° ìµœì í™”ë¥¼ í†µí•´ ë”ìš± ê²¬ê³ í•˜ê³  ì˜ë¯¸ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì¼ë°˜ ë° ì „ë¬¸ ë¶„ì•¼ì˜ í¬ë¡œìŠ¤-ëª¨ë‹¬ ê²€ìƒ‰ì—ì„œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ë°©ì‹ìœ¼ë¡œ ë‹¤ë¥¸ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì ì¸ ìƒˆë¡œìš´ ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

*   **êµ¬ì¡° ì¤‘ì‹¬ì ì¸ Fine-tuning Alignment:** ê¸°ì¡´ì˜ ì‹œê°-ì–¸ì–´ ì •ë ¬ ë°©ì‹ì´ ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ ì •ë³´ì— ì§‘ì¤‘í•˜ëŠ” ë°˜ë©´, StruXLIPëŠ” ì´ë¯¸ì§€ì˜ ì‹œê°ì  êµ¬ì¡°(edge maps)ë¥¼ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ì´ êµ¬ì¡°ì ì¸ ì •ë³´ë¥¼ ë”ìš± ì¤‘ìš”í•˜ê²Œ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
*   **êµ¬ì¡°-í…ìŠ¤íŠ¸ ì •ë ¬ ì†ì‹¤ ì¶”ê°€:** ê¸°ì¡´ì˜ CLIPê³¼ ê°™ì€ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìƒí˜¸ ì •ë³´ ìµœëŒ€í™” ì™¸ì—, edge mapê³¼ í…ìŠ¤íŠ¸, ê·¸ë¦¬ê³  ìƒ‰ìƒ ì´ë¯¸ì§€ ê°„ì˜ ì—°ê²°ì„ í†µí•´ í‘œí˜„ ë“œë¦¬í”„íŠ¸ë¥¼ ë°©ì§€í•˜ëŠ” ìƒˆë¡œìš´ êµ¬ì¡° ì¤‘ì‹¬ì ì¸ ì†ì‹¤ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
*   **ë‹¤ì¤‘ ëª¨ë‹¬ êµ¬ì¡°ì  í‘œí˜„ ìµœëŒ€í™”:** ì‹œê°ì  êµ¬ì¡°ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìƒí˜¸ ì •ë³´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë³´ì¡° ìµœì í™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì•ˆì •ì„±ê³¼ ì˜ë¯¸ì  ì¼ê´€ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì „ëµì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
*   **êµ¬ì¡°ì  ì •ë³´ í™œìš©:** ì´ë¯¸ì§€ì˜ ì‹œê°ì  êµ¬ì¡° ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ë†’ì´ê³ , íŠ¹íˆ ìƒì„¸í•˜ê³  ê¸´ ìº¡ì…˜ì„ ì²˜ë¦¬í•˜ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì„œ/ì°¨íŠ¸ VQAì™€ ê°™ì´ êµ¬ì¡°ì ì¸ ì •ë³´ê°€ ì¤‘ìš”í•œ ì‘ì—…ì— ìœ ìš©í•˜ê²Œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ ë°©ì‹:** ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê¸°ì¡´ ëª¨ë¸ì— ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì–´, ë‹¤ì–‘í•œ ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ì—°êµ¬ ê°œë°œì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
*   **ì‹¤í—˜ì  ê²€ì¦:** ì¼ë°˜ ë° ì „ë¬¸ ë¶„ì•¼ì˜ í¬ë¡œìŠ¤-ëª¨ë‹¬ ê²€ìƒ‰ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì œì•ˆí•˜ëŠ” ë°©ë²•ë¡ ì˜ íš¨ê³¼ë¥¼ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
*   **Edge Map ì¶”ì¶œ ì˜ì¡´ì„±:** Canny edge mapê³¼ ê°™ì€ edge map ì¶”ì¶œ ë°©ì‹ì— ì˜ì¡´ì ì´ë©°, ë‹¤ë¥¸ edge detection ë°©ë²•ì´ë‚˜ ì‹œê°ì  êµ¬ì¡° ì¶”ì¶œ ë°©ì‹ì— ëŒ€í•œ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
*   **êµ¬ì¡° ì¤‘ì‹¬ ì†ì‹¤ì˜ íš¨ê³¼:** êµ¬ì¡° ì¤‘ì‹¬ ì†ì‹¤ì˜ íš¨ê³¼ê°€ ëª¨ë“  ì‘ì—…ì— ë™ì¼í•˜ê²Œ ì ìš©ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, íŠ¹ì • ë°ì´í„°ì…‹ì´ë‚˜ ì‘ì—…ì— ëŒ€í•´ì„œëŠ” ì¶”ê°€ì ì¸ íŠœë‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **ìƒí˜¸ ì •ë³´ ìµœëŒ€í™”ì˜ ë³µì¡ì„±:** ë³´ì¡° ìµœì í™” ê³¼ì •ì´ ë³µì¡í•˜ë©°, ëª¨ë¸ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ì €í•´í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ì´ ë…¼ë¬¸ì€ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ Vision Encoderì™€ Text LLM ê°„ì˜ ì •ë ¬ ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë¬¸ì„œ/ì°¨íŠ¸ VQA ì—°êµ¬ì—ì„œ ì´ë¯¸ì§€ì˜ ì‹œê°ì  êµ¬ì¡° ì •ë³´ê°€ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ, StruXLIPì˜ êµ¬ì¡° ì¤‘ì‹¬ì ì¸ Fine-tuning AlignmentëŠ” ëª¨ë¸ì´ ë¬¸ì„œ/ì°¨íŠ¸ì˜ êµ¬ì¡°ë¥¼ ë” ì˜ ì´í•´í•˜ê³ , ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, í•œêµ­ì–´ Multimodal LLM ê°œë°œ ì‹œ, í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ì  íŠ¹ì§•ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë° í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
*   **ë‹¤ì–‘í•œ ì‹œê°ì  êµ¬ì¡° ì¶”ì¶œ ë°©ë²• íƒìƒ‰:** Canny edge map ì™¸ì—, Sobel, Harris ë“± ë‹¤ì–‘í•œ edge detection ë°©ë²•ì„ í™œìš©í•˜ì—¬ StruXLIPì˜ íš¨ê³¼ë¥¼ ë¹„êµ ë¶„ì„í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **êµ¬ì¡° ì¤‘ì‹¬ ì†ì‹¤ì˜ ê°€ì¤‘ì¹˜ ìµœì í™”:** êµ¬ì¡° ì¤‘ì‹¬ ì†ì‹¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°ì´í„°ì…‹ ë° ì‘ì—…ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **í•œêµ­ì–´ ë¬¸ì„œ/ì°¨íŠ¸ VQAì— ì ìš©:** í•œêµ­ì–´ ë¬¸ì„œ/ì°¨íŠ¸ VQA ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ StruXLIPë¥¼ ì ìš©í•˜ê³ , ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ë¥¼ ê²€ì¦í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **LLMê³¼ StruXLIPì˜ ê²°í•©:** LLMì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ê³¼ StruXLIPë¥¼ ê²°í•©í•˜ì—¬, ë”ìš± íš¨ê³¼ì ì¸ ë‹¤ì¤‘ ëª¨ë‹¬ ì§ˆë¬¸ ì‘ë‹µ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1.  Vision-Language Alignment
2.  Multimodal Structural Cues
3.  Edge Maps
4.  Fine-tuning Alignment
5.  Cross-Modal Retrieval

---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
