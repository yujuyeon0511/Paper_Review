---
title: "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator"
date: 2026-02-08
arxiv: "2602.05552v1"
category: "cs.RO"
model: "google/gemma-3-27b-it:free"
---

# VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Bessie Dominguez-Dager, Sergio Suescun-Ferrandiz, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla |
| **ë°œí‘œì¼** | 2026-02-05 |
| **arXiv** | [2602.05552v1](https://arxiv.org/pdf/2602.05552v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.RO |

---

## ğŸ“ ì´ˆë¡ (Abstract)

This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

---

## ğŸ” AI ë¶„ì„

## VLN-Pilot ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ GPSê°€ ì œí•œëœ ì‹¤ë‚´ í™˜ê²½ì—ì„œ ë“œë¡  í•­ë²•ì„ ìœ„í•´ ëŒ€ê·œëª¨ Vision-and-Language ëª¨ë¸(VLLM)ì„ ì¸ê°„ ì¡°ì¢…ì‚¬ì²˜ëŸ¼ í™œìš©í•˜ëŠ” VLN-Pilot í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ í•´ì„í•˜ê³  ì‹œê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“œë¡ ì˜ ê¶¤ì ì„ ê³„íš ë° ì‹¤í–‰í•˜ë©°, ê¸°ì¡´ì˜ ê·œì¹™ ê¸°ë°˜ ë˜ëŠ” ê¸°í•˜í•™ì  ê²½ë¡œ ê³„íš ë°©ì‹ê³¼ ë‹¬ë¦¬ ì–¸ì–´ ê¸°ë°˜ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ì™€ ì‹œê° ì¸ì‹ì„ í†µí•©í•©ë‹ˆë‹¤. VLN-PilotëŠ” ê³µê°„ ê´€ê³„, ì¥ì• ë¬¼ íšŒí”¼, ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì— ëŒ€í•œ ë™ì  ë°˜ì‘ì„ ì¶”ë¡ í•˜ì—¬ ì™„ì „ ììœ¨ì ì¸ ëª…ë ¹ ì¶”ì¢…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì‹¤ë‚´ UAV ì œì–´ì˜ í™•ì¥ì„±, ì‚¬ìš©ì ì¹œí™”ì„±, ì•ˆì „ì„±ì„ í–¥ìƒì‹œí‚¬ ì ì¬ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
VLN-PilotëŠ” ê¸°ì¡´ ë“œë¡  í•­ë²• ì—°êµ¬ì™€ ì°¨ë³„í™”ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒˆë¡œìš´ ì ì„ ì œì‹œí•©ë‹ˆë‹¤.

*   **VLLM ê¸°ë°˜ ììœ¨ í•­ë²•:** ê¸°ì¡´ì˜ ë“œë¡  í•­ë²• ì‹œìŠ¤í…œì´ ê·œì¹™ ê¸°ë°˜ ë˜ëŠ” ê¸°í•˜í•™ì  ê²½ë¡œ ê³„íšì— ì˜ì¡´í–ˆë˜ ë°˜ë©´, ë³¸ ë…¼ë¬¸ì€ VLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ ì§ì ‘ í•´ì„í•˜ê³  ì‹¤í–‰í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤.
*   **ì–¸ì–´ ê¸°ë°˜ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ í†µí•©:** ì‹œê° ì •ë³´ì™€ í•¨ê»˜ ìì—°ì–´ ëª…ë ¹ì˜ ì˜ë¯¸ë¡ ì  ì´í•´ë¥¼ í†µí•©í•˜ì—¬, ë³´ë‹¤ ìœ ì—°í•˜ê³  ë§¥ë½ ì¸ì‹ì ì¸ ë“œë¡  ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
*   **ì‹¤ë‚´ í™˜ê²½ì— íŠ¹í™”ëœ ì„±ëŠ¥:** GPSê°€ ì œí•œëœ ì‹¤ë‚´ í™˜ê²½ì—ì„œ VLLM ê¸°ë°˜ì˜ ììœ¨ í•­ë²• ì„±ëŠ¥ì„ ì…ì¦í•˜ë©°, ê¸°ì¡´ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1.  **ë†’ì€ ìˆ˜ì¤€ì˜ ììœ¨ì„±:** VLLMì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ ìì—°ì–´ ëª…ë ¹ì„ ì´í•´í•˜ê³  ì‹¤í–‰í•¨ìœ¼ë¡œì¨, ë“œë¡ ì˜ ì™„ì „ ììœ¨ì ì¸ ëª…ë ¹ ì¶”ì¢…ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
2.  **ìœ ì—°ì„±ê³¼ ì ì‘ì„±:** ì–¸ì–´ ê¸°ë°˜ì˜ ì œì–´ ë°©ì‹ì„ í†µí•´, ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ì‘ì—…ì— ìœ ì—°í•˜ê²Œ ì ì‘í•  ìˆ˜ ìˆìœ¼ë©°, ìƒˆë¡œìš´ ì‘ì—…ì— ëŒ€í•œ ì—”ì§€ë‹ˆì–´ë§ ë…¸ë ¥ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
3.  **ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±:** ì‹¤ë‚´ í™˜ê²½ì—ì„œì˜ ë†’ì€ ì„±ê³µë¥ ì„ ì…ì¦í•˜ë©°, ê²€ì‚¬, ìˆ˜ìƒ‰ ë° êµ¬ì¡°, ì‹œì„¤ ëª¨ë‹ˆí„°ë§ ë“± ë‹¤ì–‘í•œ ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì— ì ìš©ë  ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1.  **ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì˜ì¡´ì„±:** ì‹¤í—˜ ê²°ê³¼ê°€ photorealistic ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ì–»ì–´ì§„ ê²ƒìœ¼ë¡œ, ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì˜ ë³µì¡ì„±ê³¼ ë…¸ì´ì¦ˆëŠ” VLLMì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2.  **VLLMì˜ í•œê³„:** VLLM ìì²´ì˜ í•œê³„ (í™˜ê°, ì¶”ë¡  ì˜¤ë¥˜ ë“±)ê°€ ë“œë¡ ì˜ ì˜¤ì‘ë™ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì•ˆì „ì´ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œëŠ” VLLMì˜ ì‹ ë¢°ì„±ì„ í™•ë³´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
3.  **ê³„ì‚° ë¹„ìš©:** VLLMì€ ì¼ë°˜ì ìœ¼ë¡œ ê³„ì‚° ë¹„ìš©ì´ ë†’ê¸° ë•Œë¬¸ì—, ì‹¤ì‹œê°„ ì œì–´ì— í•„ìš”í•œ ì—°ì‚° ëŠ¥ë ¥ì„ í™•ë³´í•˜ëŠ” ê²ƒì´ ê³¼ì œì…ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì €ì˜ ì—°êµ¬ ë¶„ì•¼ì¸ Scientific MLLM, Reasoning MLLM, Vision-Language Alignmentì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ìŒê³¼ ê°™ì€ ì¸¡ë©´ì—ì„œ ì—°ê´€ì„±ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

*   **Vision-Language Alignment:** VLN-PilotëŠ” ì‹œê° ì •ë³´ì™€ ìì—°ì–´ ëª…ë ¹ ê°„ì˜ ì •ë ¬(alignment)ì„ í†µí•´ ë“œë¡ ì„ ì œì–´í•©ë‹ˆë‹¤. ì´ëŠ” ì €ì˜ ì—°êµ¬ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ì£¼ì œì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.
*   **Multimodal Reasoning:** VLLMì˜ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ëŠ¥ë ¥ì€ ë“œë¡ ì˜ ììœ¨ í•­ë²•ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì €ì˜ ì—°êµ¬ëŠ” MLLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©°, VLN-Pilotì˜ ì„±ê³µì€ ì´ëŸ¬í•œ ì—°êµ¬ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤.
*   **Scientific ë„ë©”ì¸ ì ìš©:** VLN-Pilotì˜ ì‘ìš© ë¶„ì•¼ì¸ ê²€ì‚¬, ìˆ˜ìƒ‰ ë° êµ¬ì¡°, ì‹œì„¤ ëª¨ë‹ˆí„°ë§ì€ ê³¼í•™ì ì¸ ë„ë©”ì¸ê³¼ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì €ì˜ ì—°êµ¬ê°€ Scientific MLLMì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤ëŠ” ì ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
ë³¸ ë…¼ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì—°êµ¬ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

1.  **ì‹¤ì œ í™˜ê²½ì—ì„œì˜ VLN-Pilot ì„±ëŠ¥ í‰ê°€:** ì‹¤ì œ ì‹¤ë‚´ í™˜ê²½ì—ì„œ VLN-Pilotì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ê³¼ì˜ ì°¨ì´ì ì„ ë¶„ì„í•©ë‹ˆë‹¤.
2.  **VLLMì˜ ì‹ ë¢°ì„± í–¥ìƒ:** VLLMì˜ í™˜ê° ë° ì¶”ë¡  ì˜¤ë¥˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë°©ë²•ì„ ì—°êµ¬í•˜ê³ , ë“œë¡  ì œì–´ ì‹œìŠ¤í…œì˜ ì•ˆì „ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, VLLMì˜ ì˜ˆì¸¡ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•˜ê³ , ì•ˆì „ ë§ˆì§„ì„ í™•ë³´í•˜ëŠ” ë°©ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3.  **Scientific ë„ë©”ì¸ íŠ¹í™”ëœ VLN-Pilot ê°œë°œ:** íŠ¹ì • ê³¼í•™ì  ë„ë©”ì¸ (ì˜ˆ: í™”í•™ ì‹¤í—˜ì‹¤, ìƒë¬¼í•™ ì—°êµ¬ì‹¤)ì— íŠ¹í™”ëœ VLN-Pilotë¥¼ ê°œë°œí•˜ê³ , í•´ë‹¹ ë„ë©”ì¸ì˜ ìš”êµ¬ ì‚¬í•­ì— ë§ëŠ” VLLMì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
4.  **Vision Encoder ê°œì„ ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ:** VLN-Pilotì— ì‚¬ìš©ë˜ëŠ” Vision Encoderì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ì—¬, ì‹œê° ì •ë³´ì˜ ì •í™•ì„±ê³¼ í’ë¶€ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, self-supervised learningì„ í†µí•´ ë” ê°•ë ¥í•œ Vision Encoderë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1.  Vision-Language Model (VLLM)
2.  Autonomous Drone Navigation
3.  Multimodal Reasoning
4.  Indoor Localization
5.  Instruction Following

---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
