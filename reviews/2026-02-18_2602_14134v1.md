---
title: "DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors"
date: 2026-02-18
arxiv: "2602.14134v1"
category: "cs.CV"
model: "google/gemma-3-27b-it:free"
---

# DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Yi Li, Hongze Shen, Lexiang Tang, Xin Li, Xinpeng Ding... |
| **ë°œí‘œì¼** | 2026-02-15 |
| **arXiv** | [2602.14134v1](https://arxiv.org/pdf/2602.14134v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.CV |

---

## ğŸ“ ì´ˆë¡ (Abstract)

Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

---

## ğŸ” AI ë¶„ì„

## DenseMLLM ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ ê¸°ì¡´ MLLMì´ ê³ ìˆ˜ì¤€ ì‹œê° ì´í•´ì—ëŠ” ë›°ì–´ë‚˜ì§€ë§Œ, ì˜ë¯¸ë¡ ì  ë¶„í• ì´ë‚˜ ê¹Šì´ ì¶”ì • ê°™ì€ ì„¸ë°€í•œ ì˜ˆì¸¡ ì‘ì—…ì—ëŠ” ë³µì¡í•œ ë””ì½”ë”ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì ì— ë„ì „í•©ë‹ˆë‹¤. DenseMLLMì€ ì¶”ê°€ì ì¸ task-specific ë””ì½”ë” ì—†ì´ í‘œì¤€ MLLM ì•„í‚¤í…ì²˜ë¥¼ í™œìš©í•˜ì—¬ dense predictionì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì œì•ˆí•©ë‹ˆë‹¤. ìƒˆë¡œìš´ vision token supervision ì „ëµì„ í†µí•´ ë‹¤ì–‘í•œ ë¼ë²¨ê³¼ taskì— ì ìš© ê°€ëŠ¥í•˜ë©°, ë‹¨ìˆœí•œ êµ¬ì¡°ì—ë„ ë¶ˆêµ¬í•˜ê³  ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” í‘œì¤€ MLLMì´ ì•„í‚¤í…ì²˜ íŠ¹í™” ì—†ì´ë„ dense perceptionì„ íš¨ê³¼ì ìœ¼ë¡œ ì§€ì›í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
* **í‘œì¤€ MLLM ì•„í‚¤í…ì²˜ í™œìš©:** ê¸°ì¡´ ì—°êµ¬ë“¤ì´ dense predictionì„ ìœ„í•´ ë³µì¡í•œ ë””ì½”ë”ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, DenseMLLMì€ í‘œì¤€ MLLM ì•„í‚¤í…ì²˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ì„œ vision token supervision ì „ëµì„ í†µí•´ dense predictionì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
* **Vision Token Supervision:** Multiple labelsì™€ tasksì— ëŒ€í•œ ìƒˆë¡œìš´ vision token supervision ì „ëµì„ ì œì‹œí•˜ì—¬, MLLMì´ dense predictionì„ ìˆ˜í–‰í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
* **Generalist MLLMì˜ ê°€ëŠ¥ì„± ì œì‹œ:**  íŠ¹ì • taskì— íŠ¹í™”ëœ êµ¬ì¡° ì—†ì´ë„ MLLMì´ dense perceptionì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œìœ¼ë¡œì¨, generalist MLLMì˜ ì ì¬ë ¥ì„ ì…ì¦í•©ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ë‹¨ìˆœì„±:** ë³µì¡í•œ ì•„í‚¤í…ì²˜ ë³€ê²½ ì—†ì´ í‘œì¤€ MLLMì„ í™œìš©í•˜ì—¬ êµ¬í˜„ì´ ìš©ì´í•˜ê³ , ëª¨ë¸ ë³µì¡ì„±ì„ ì¤„ì…ë‹ˆë‹¤.
2. **ë²”ìš©ì„±:** ë‹¤ì–‘í•œ dense prediction taskì™€ vision-language benchmarkì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ë‹¤ì–‘í•œ taskì— ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
3. **íš¨ìœ¨ì„±:** task-specific ë””ì½”ë” ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì—¬, ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¦ê°€ ì—†ì´ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¹ë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
* **Vision Token Supervisionì˜ êµ¬ì²´ì ì¸ ë©”ì»¤ë‹ˆì¦˜:** Vision token supervision ì „ëµì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ì‹ê³¼ íš¨ê³¼ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ tokenì„ supervisioní•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ê·¸ íš¨ê³¼ê°€ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.
* **í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ ë¶€ì¬:**  ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì˜ì–´ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ê°€ ì—†ì–´, í•œêµ­ì–´ MLLMì— ëŒ€í•œ ì ìš© ê°€ëŠ¥ì„±ì„ íŒë‹¨í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
* **ì‹¤ì‹œê°„ ì„±ëŠ¥:** Dense predictionì€ ê³„ì‚°ëŸ‰ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. DenseMLLMì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ì— ëŒ€í•œ ì–¸ê¸‰ì´ ì—†ì–´, ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì˜ë¬¸ì´ ë‚¨ìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì œ ì—°êµ¬ ë¶„ì•¼ì¸ MLLMì˜ vision-text alignment ë° ì„±ëŠ¥ í–¥ìƒê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, vision encoderì™€ text LLM ê°„ì˜ ì •ë ¬ ì—°êµ¬ì— ìˆì–´, DenseMLLMì˜ vision token supervision ì „ëµì€ ìƒˆë¡œìš´ alignment ë°©ë²•ì„ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, MLLM mergingì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ì— ìˆì–´, DenseMLLMì˜ ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì¸ êµ¬ì¡°ëŠ” merging ê³¼ì •ì—ì„œ ëª¨ë¸ ë³µì¡ì„±ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ MLLM ê°œë°œ ë° í‰ê°€ ì¸¡ë©´ì—ì„œëŠ”, DenseMLLMì˜ ì•„ì´ë””ì–´ë¥¼ í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê³ , í•œêµ­ì–´ íŠ¹ì„±ì— ë§ëŠ” vision token supervision ì „ëµì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **í•œêµ­ì–´ ë°ì´í„°ì…‹ ê¸°ë°˜ DenseMLLM ì„±ëŠ¥ í‰ê°€:** í•œêµ­ì–´ ë°ì´í„°ì…‹(KU-VQA, KorDocQA ë“±)ì„ í™œìš©í•˜ì—¬ DenseMLLMì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , í•œêµ­ì–´ MLLMì— ëŒ€í•œ ì ìš© ê°€ëŠ¥ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
2. **Vision Token Supervision ì „ëµ ê°œì„ :** í•œêµ­ì–´ ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ vision token supervision ì „ëµì„ ê°œì„ í•˜ê³ , ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ì„ ëª¨ìƒ‰í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ token supervisionì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **DenseMLLMê³¼ MLLM Merging ê²°í•©:** DenseMLLMì˜ êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ MLLM mergingì„ ìˆ˜í–‰í•˜ê³ , ì„±ëŠ¥ í–¥ìƒ ë° ëª¨ë¸ ë³µì¡ì„± ê°ì†Œ íš¨ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
4. **DocVQA/ChartQA/Table VQAì— DenseMLLM ì ìš©:** DenseMLLMì„ ë¬¸ì„œ, ì°¨íŠ¸, í…Œì´ë¸” ì´í•´ì— íŠ¹í™”ëœ MLLMì— ì ìš©í•˜ì—¬, í•´ë‹¹ taskì—ì„œì˜ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ì„±ì„ íƒìƒ‰í•©ë‹ˆë‹¤. íŠ¹íˆ, í…Œì´ë¸” êµ¬ì¡°ë¥¼ vision tokenìœ¼ë¡œ í‘œí˜„í•˜ê³  supervisioní•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Dense Prediction
2. Vision-Language Alignment
3. Multimodal Large Language Models (MLLM)
4. Vision Token Supervision
5. Generalist Model

---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
