---
title: "PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Mult"
date: 2026-02-16
arxiv: "2602.12597v1"
category: "cs.RO"
model: "google/gemma-3-4b-it:free"
---

# PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Mahdi Haghighat Joo, Maryam Karimi Jafari, Alireza Taheri |
| **ë°œí‘œì¼** | 2026-02-13 |
| **arXiv** | [2602.12597v1](https://arxiv.org/pdf/2602.12597v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.RO |

---

## ğŸ“ ì´ˆë¡ (Abstract)

This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.

---

## ğŸ” AI ë¶„ì„

## ë…¼ë¬¸ ë¶„ì„: PISHYAR - ì‚¬íšŒì  ì§€ëŠ¥í˜• ìŠ¤ë§ˆíŠ¸ ì§€íŒ¡ì´

### ğŸ“„ ë…¼ë¬¸ ìš”ì•½
PISHYARëŠ” ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì‚¬íšŒì  ì§€ëŠ¥í˜• ìŠ¤ë§ˆíŠ¸ ì§€íŒ¡ì´ ì‹œìŠ¤í…œìœ¼ë¡œ, ì‹¤ì‹œê°„ RGB-D ì„¼ì„œ, ê°ì²´ íƒì§€, êµ°ì§‘ í™œë™ ì¸ì‹, ë™ì  ê²½ë¡œ ê³„íš, ì§„ë™ í”¼ë“œë°±ì„ í†µí•©í•˜ì—¬ ì‚¬íšŒì  ì´ë™ ì§€ì›ê³¼ ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ê°„-ë¡œë´‡ ìƒí˜¸ ì‘ìš©ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ìŒì„± ì¸ì‹, ë¹„ì „ ì–¸ì–´ ëª¨ë¸, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸, í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ê¸°ë°˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, ì¥ë©´ ì„¤ëª…, ì‹œê°ì  ì…ë ¥ ê¸°ë°˜ ê°ì²´ ìœ„ì¹˜ íŒŒì•… ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜, í˜„ì¥ ì‹¤í—˜, ì‚¬ìš©ì ì¤‘ì‹¬ ì—°êµ¬ë¥¼ í†µí•´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í‰ê°€í–ˆìœ¼ë©°, 80% ìˆ˜ì¤€ì˜ ì „ë°˜ì ì¸ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ êµ°ì§‘ í™œë™ ì¸ì‹ ì„±ëŠ¥ ë˜í•œ ìš°ìˆ˜í•˜ë©°, ì´ˆê¸° ì‚¬ìš©ì ì—°êµ¬ ê²°ê³¼ ë†’ì€ ì‚¬ìš© ìˆ˜ìš©ë„ì™€ ì‹ ë¢°ì„±, ì‚¬íšŒì„± ì¸ì‹ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ì´ ë…¼ë¬¸ì€ ë‹¨ìˆœíˆ ì´ë™ ì§€ì› ê¸°ëŠ¥ì„ ë„˜ì–´, ì‚¬íšŒì  ë§¥ë½ì„ ê³ ë ¤í•œ ë‹¤ì¤‘ ëª¨ë‹¬ ìƒí˜¸ ì‘ìš©ì„ ì œê³µí•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ì§€íŒ¡ì´ ì‹œìŠ¤í…œì„ ì œì‹œí•˜ëŠ” ì ì´ ê°€ì¥ í° íŠ¹ì§•ì…ë‹ˆë‹¤. íŠ¹íˆ, RGB-D ì„¼ì„œ, ê°ì²´ íƒì§€, êµ°ì§‘ í™œë™ ì¸ì‹, ë™ì  ê²½ë¡œ ê³„íš, ê·¸ë¦¬ê³  LLM-VLM í†µí•©ì„ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œìœ¼ë¡œ ìœµí•©í•˜ì—¬ ì‹œê° ì¥ì• ì¸ì˜ ì‚¬íšŒì  ì´ë™ ë° ìƒí˜¸ ì‘ìš©ì„ ì§€ì›í•˜ëŠ” ì‹œë„ê°€ ë‹ë³´ì…ë‹ˆë‹¤. ë˜í•œ, ìŒì„±-ì‹œê° ëª¨ë“œ ì „í™˜ì„ í†µí•œ ìœ ì—°í•œ ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„ì™€ UTAUT ê¸°ë°˜ì˜ ì‚¬ìš©ì ìˆ˜ìš©ì„± í‰ê°€ë¥¼ í†µí•´ ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì—ì„œì˜ ì ìš© ê°€ëŠ¥ì„±ì„ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **í†µí•©ì ì¸ ë‹¤ì¤‘ ëª¨ë‹¬ ì‹œìŠ¤í…œ:** RGB-D ì„¼ì„œ, ê°ì²´ íƒì§€, êµ°ì§‘ í™œë™ ì¸ì‹, LLM-VLM ë“± ë‹¤ì–‘í•œ ê¸°ìˆ ì„ í†µí•©í•˜ì—¬ ì‹œê° ì¥ì• ì¸ì˜ ë³µì¡í•œ í™˜ê²½ì—ì„œ íš¨ê³¼ì ì¸ ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤.
2. **ì‚¬íšŒì  ë§¥ë½ ê³ ë ¤:** ë‹¨ìˆœíˆ ê²½ë¡œ ì•ˆë‚´ë¿ë§Œ ì•„ë‹ˆë¼, êµ°ì§‘ í™œë™ ì¸ì‹ê³¼ ê°™ì€ ì‚¬íšŒì  ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì ì ˆí•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì‚¬íšŒì  ìƒí˜¸ ì‘ìš©ì„ ì§€ì›í•©ë‹ˆë‹¤.
3. **ì‚¬ìš©ì ì¤‘ì‹¬ í‰ê°€:** ì‹œë®¬ë ˆì´ì…˜, í˜„ì¥ ì‹¤í—˜, ì‚¬ìš©ì ì¤‘ì‹¬ ì—°êµ¬ë¥¼ í†µí•´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ê³¼ ì‚¬ìš©ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , UTAUT ê¸°ë°˜ì˜ ì‚¬ìš©ì ìˆ˜ìš©ì„± í‰ê°€ë¥¼ í†µí•´ ì‹¤ì œ ì‚¬ìš©ìì˜ ë§Œì¡±ë„ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **LLM-VLM ì„±ëŠ¥ ì˜ì¡´ì„±:** LLM-VLMì˜ ì„±ëŠ¥ì— ë”°ë¼ ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì´ í¬ê²Œ ì¢Œìš°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ë³µì¡í•œ ì¥ë©´ ì„¤ëª…ì´ë‚˜ ê°ì²´ ìœ„ì¹˜ íŒŒì•… ì‹œ LLMì˜ í•œê³„ê°€ ë“œëŸ¬ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ì‹¤ì œ í™˜ê²½ ì ìš©ì˜ ì–´ë ¤ì›€:** RGB-D ì„¼ì„œì™€ ê°™ì€ í•˜ë“œì›¨ì–´ì˜ ì„±ëŠ¥ ë° ë¹„ìš©, ê·¸ë¦¬ê³  LLMì˜ ì—°ì‚°ëŸ‰ ë“±ì„ ê³ ë ¤í•  ë•Œ ì‹¤ì œ í™˜ê²½ ì ìš©ì— ì–´ë ¤ì›€ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ì´ˆê¸° ì‚¬ìš©ì ì—°êµ¬ì˜ í•œê³„:** 8ëª…ì˜ ì°¸ê°€ìë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ì´ˆê¸° ì‚¬ìš©ì ì—°êµ¬ëŠ” ì‹œìŠ¤í…œì˜ ì ì¬ë ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ë” ë§ì€ ì‚¬ìš©ìë“¤ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ê´‘ë²”ìœ„í•œ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì˜ ì—°êµ¬ëŠ” MLLMì„ í™œìš©í•œ ë‹¤ì¤‘ ëª¨ë‹¬ VQA ë° Vision-Text Alignment ì—°êµ¬ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì‹œê°ì  ì…ë ¥(RGB-D ì„¼ì„œ, ê°ì²´ íƒì§€)ì„ LLM-VLMì— ì œê³µí•˜ì—¬ ì¥ë©´ ì„¤ëª… ë° ê°ì²´ ìœ„ì¹˜ íŒŒì•…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì€ í˜„ì¬ ì €ì˜ ì—°êµ¬ ì£¼ì œì™€ ì¼ì¹˜í•©ë‹ˆë‹¤. ë˜í•œ, í•œêµ­ì–´ MLLM ê°œë°œ ë° í‰ê°€ ì—°êµ¬ì— í™œìš©ë  ìˆ˜ ìˆëŠ” ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.  PISHYARì˜ êµ°ì§‘ í™œë™ ì¸ì‹ ê¸°ìˆ ì€ ë³µì¡í•œ ì‚¬íšŒì  í™˜ê²½ì—ì„œ MLLMì´ ìƒí™©ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **LLM-VLM ì„±ëŠ¥ ê°œì„ :** ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ LLM-VLMì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , íŠ¹íˆ ë³µì¡í•œ ì¥ë©´ ì„¤ëª… ë° ê°ì²´ ìœ„ì¹˜ íŒŒì•… ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ì‹¤ì‹œê°„ ê°ì²´ ì¸ì‹ ë° ì¶”ì :** ì‹¤ì‹œê°„ ê°ì²´ ì¸ì‹ ë° ì¶”ì  ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ PISHYARì˜ ìƒí™© ì¸ì§€ ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **í•œêµ­ì–´ ê¸°ë°˜ì˜ ë‹¤ì¤‘ ëª¨ë‹¬ VQA ì‹œìŠ¤í…œ ê°œë°œ:** í•œêµ­ì–´ MLLMì„ í™œìš©í•˜ì—¬ PISHYARì˜ í•œêµ­ì–´ ê¸°ë°˜ ë‹¤ì¤‘ ëª¨ë‹¬ VQA ì‹œìŠ¤í…œì„ ê°œë°œí•˜ê³ , ì‹¤ì œ ì‹œê° ì¥ì• ì¸ì˜ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **ë‹¤ì–‘í•œ ì„¼ì„œ ìœµí•©:** LiDAR, ì´ˆìŒíŒŒ ì„¼ì„œ ë“± ë‹¤ì–‘í•œ ì„¼ì„œë¥¼ ìœµí•©í•˜ì—¬ PISHYARì˜ í™˜ê²½ ì¸ì‹ ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. **ì‚¬ìš©ì ë§ì¶¤í˜• ì¸í„°í˜ì´ìŠ¤ ê°œë°œ:** ì‚¬ìš©ìë³„ íŠ¹ì„±(ì‹œê° ì¥ì•  ìœ í˜•, ì‚¬ìš© ê²½í—˜ ë“±)ì„ ê³ ë ¤í•˜ì—¬ PISHYARì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©ì ë§ì¶¤í˜•ìœ¼ë¡œ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. MLLM (Multimodal Large Language Models)
2. Vision-Text Alignment
3. Document VQA (Document Visual Question Answering)
4. Social Navigation
5. Agentic Interaction


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-4b-it:free
