---
title: "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs"
date: 2026-02-12
arxiv: "2602.08241v1"
category: "cs.AI"
model: "google/gemma-3-27b-it:free"
---

# Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs

## ğŸ“– ë…¼ë¬¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì €ì** | Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li |
| **ë°œí‘œì¼** | 2026-02-09 |
| **arXiv** | [2602.08241v1](https://arxiv.org/pdf/2602.08241v1) |
| **ì¹´í…Œê³ ë¦¬** | cs.AI |

---

## ğŸ“ ì´ˆë¡ (Abstract)

While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

---

## ğŸ” AI ë¶„ì„

## ğŸ“„ ë…¼ë¬¸ ìš”ì•½
ë³¸ ë…¼ë¬¸ì€ Multimodal Large Language Models (MLLM)ì˜ ì‹œê°ì  ì£¼ì˜(visual attention) ë©”ì»¤ë‹ˆì¦˜ì˜ í•œê³„ë¥¼ ì§€ì í•˜ë©°, íŠ¹íˆ ì¶”ë¡  ê³¼ì •ì—ì„œ ì´ˆê¸° ì‹œê°ì  ì˜¤ì •ë ¬ì´ ìˆ˜ì •ë˜ì§€ ì•Šì•„ ì˜¤ë¥˜ê°€ ì „íŒŒë˜ëŠ” ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°•í™”í•™ìŠµ(RL) ê¸°ë°˜ì˜ SAYO ëª¨ë¸ì„ ì œì•ˆí•˜ë©°, ì˜ì—­ ìˆ˜ì¤€ì˜ ì‹œê°ì  ì£¼ì˜ì— ëŒ€í•œ ë³´ìƒ(reward)ì„ ë„ì…í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ê·¼ê±°ëœ ì¶”ë¡  ë‹¨ê³„ë¥¼ ê°•í™”í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, SAYOëŠ” ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, ì•ˆì •ì ì¸ ì‹œê°ì  ì£¼ì˜ í•™ìŠµì˜ ì¤‘ìš”ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

### ğŸ†• ìƒˆë¡œìš´ ì  (Novelty)
ê¸°ì¡´ MLLM ì—°êµ¬ë“¤ì´ ì£¼ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ì¶”ë¡  ê²½ë¡œì— ì§‘ì¤‘í•œ ë°˜ë©´, ë³¸ ë…¼ë¬¸ì€ **ì‹œê°ì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ ìì²´ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹**ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, ê°•í™”í•™ìŠµì„ í™œìš©í•˜ì—¬ ì‹œê°ì  ì£¼ì˜ì— ì§ì ‘ì ì¸ ë³´ìƒì„ ì œê³µí•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ ì‹œê°ì  ì •ë³´ì— ë”ìš± ì§‘ì¤‘í•˜ê³  ì •í™•í•œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì ì´ ìƒˆë¡­ìŠµë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì´ CoTë¥¼ í†µí•´ ê°„ì ‘ì ìœ¼ë¡œ ì‹œê°ì  ì •ë³´ë¥¼ í™œìš©í•œ ë°˜ë©´, SAYOëŠ” **ëª…ì‹œì ì¸ ì‹œê°ì  ì£¼ì˜ í•™ìŠµ**ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤ëŠ” ì ì´ ì°¨ë³„ì ì…ë‹ˆë‹¤.

### ğŸ’ª ê°•ì  (Strengths)
1. **ë¬¸ì œ ì •ì˜ì˜ ëª…í™•ì„±:** MLLMì˜ ì‹œê°ì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì·¨ì•½ì ì„ ëª…í™•í•˜ê²Œ ë¶„ì„í•˜ê³ , ì´ˆê¸° ì˜¤ì •ë ¬ ë¬¸ì œì™€ ì˜¤ë¥˜ ì „íŒŒ í˜„ìƒì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí–ˆìŠµë‹ˆë‹¤.
2. **ê°•í™”í•™ìŠµ ê¸°ë°˜ì˜ íš¨ê³¼ì ì¸ ì ‘ê·¼:** ê°•í™”í•™ìŠµì„ í†µí•´ ì‹œê°ì  ì£¼ì˜ì— ì§ì ‘ì ì¸ ë³´ìƒì„ ì œê³µí•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ ì‹œê°ì  ì •ë³´ì— ë”ìš± ì§‘ì¤‘í•˜ë„ë¡ ìœ ë„í•˜ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.
3. **ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ ì„±ëŠ¥ ê²€ì¦:** ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SAYOì˜ ì„±ëŠ¥ í–¥ìƒì„ ì…ì¦í•˜ì—¬, ì œì•ˆí•˜ëŠ” ë°©ë²•ì˜ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì•½ì /í•œê³„ì  (Limitations)
1. **ê°•í™”í•™ìŠµì˜ ë¶ˆì•ˆì •ì„±:** ê°•í™”í•™ìŠµì€ í•™ìŠµ ê³¼ì •ì´ ë¶ˆì•ˆì •í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì— ë¯¼ê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¶€ë¶„ì„ ì–´ë–»ê²Œ ê·¹ë³µí–ˆëŠ”ì§€ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
2. **ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ì˜ ì–´ë ¤ì›€:** ì˜ì—­ ìˆ˜ì¤€ì˜ ì‹œê°ì  ì£¼ì˜ì— ëŒ€í•œ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì„¤ê³„í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë©°, ë³´ìƒ í•¨ìˆ˜ì˜ ì„¤ê³„ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **í•œêµ­ì–´ ë°ì´í„°ì— ëŒ€í•œ ê³ ë ¤ ë¶€ì¡±:** ë³¸ ë…¼ë¬¸ì€ ì£¼ë¡œ ì˜ì–´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ ê²€ì¦ì€ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### ğŸ”— ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê´€ì„±
ë³¸ ë…¼ë¬¸ì€ ì œ ì—°êµ¬ ë¶„ì•¼ì¸ **Vision-Text Alignment**ì™€ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, MLLMì˜ ì‹œê°ì  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê°•í™”í•˜ëŠ” SAYOì˜ ì ‘ê·¼ ë°©ì‹ì€, ì œê°€ ì—°êµ¬í•˜ê³  ìˆëŠ” ì‹œê° ì¸ì½”ë”ì™€ í…ìŠ¤íŠ¸ LLM ê°„ì˜ ì •ë ¬(alignment) ì—°êµ¬ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, **Document/Chart/OCR/Table VQA** ì—°êµ¬ì— ìˆì–´ì„œ, ì •í™•í•œ ì‹œê°ì  ì •ë³´ ì´í•´ëŠ” í•„ìˆ˜ì ì´ë©°, SAYOì˜ ì‹œê°ì  ì£¼ì˜ ê°•í™” ë°©ë²•ì€ ì´ëŸ¬í•œ ë¶„ì•¼ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, **Korean MLLM** ê°œë°œ ì‹œ, í•œêµ­ì–´ ë°ì´í„°ì— íŠ¹í™”ëœ ì‹œê°ì  ì£¼ì˜ í•™ìŠµ ë°©ë²•ì„ ì ìš©í•œë‹¤ë©´ ë”ìš± íš¨ê³¼ì ì¸ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

### ğŸ’¡ ì—°êµ¬ ì•„ì´ë””ì–´ ì œì•ˆ
1. **í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ í™œìš©í•œ SAYO ì„±ëŠ¥ ê²€ì¦:** í•œêµ­ì–´ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹(ì˜ˆ: KorVQA, KoDocVQA)ì„ êµ¬ì¶•í•˜ê³ , SAYO ëª¨ë¸ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
2. **SAYOì™€ ëª¨ë¸ ë¨¸ì§•(Model Merging) ê²°í•©:** SAYOë¥¼ í†µí•´ ì‹œê°ì  ì£¼ì˜ë¥¼ ê°•í™”í•œ ëª¨ë¸ì„ ë‹¤ë¥¸ MLLMê³¼ ë¨¸ì§•í•˜ì—¬, ë”ìš± ê°•ë ¥í•œ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ì„ ê°œë°œí•©ë‹ˆë‹¤.
3. **ì‹œê°ì  ì£¼ì˜ ë³´ìƒ í•¨ìˆ˜ ê°œì„ :** ì˜ì—­ ìˆ˜ì¤€ì˜ ì‹œê°ì  ì£¼ì˜ ë³´ìƒ í•¨ìˆ˜ë¥¼ ê°œì„ í•˜ì—¬, ë”ìš± ì •êµí•˜ê³  íš¨ê³¼ì ì¸ ì‹œê°ì  ì£¼ì˜ í•™ìŠµì„ ìœ ë„í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê°ì²´ ê²€ì¶œ(Object Detection) ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **DocVQA/ChartQA/Table VQAì— íŠ¹í™”ëœ SAYO ì ìš©:** ë¬¸ì„œ, ì°¨íŠ¸, í…Œì´ë¸” ì´í•´ì— íŠ¹í™”ëœ SAYO ëª¨ë¸ì„ ê°œë°œí•˜ê³ , í•´ë‹¹ ë¶„ì•¼ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### ğŸ“š í•µì‹¬ í‚¤ì›Œë“œ
1. Multimodal Large Language Models (MLLM)
2. Visual Attention
3. Reinforcement Learning (RL)
4. Vision-Text Alignment
5. Visual Reasoning


---

> ğŸ¤– ì´ ê¸€ì€ AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ë¶„ì„ ëª¨ë¸: google/gemma-3-27b-it:free
